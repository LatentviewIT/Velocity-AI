{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Main Code"
      ],
      "metadata": {
        "id": "bjU7yc3ANkH_"
      },
      "id": "bjU7yc3ANkH_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Libraries"
      ],
      "metadata": {
        "id": "-nOP2Av89eWT"
      },
      "id": "-nOP2Av89eWT"
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import NotFound\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import timedelta"
      ],
      "metadata": {
        "id": "edbUQVlo9cqc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1771541088277,
          "user_tz": 360,
          "elapsed": 4629,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "57ef8821-8b06-4224-de8a-c3d79b23626b"
      },
      "id": "edbUQVlo9cqc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.cloud.resourcemanager_v3 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.resourcemanager_v3 past that date.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.cloud.bigquery_storage_v1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.bigquery_storage_v1 past that date.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Configuration & Naming Standards"
      ],
      "metadata": {
        "id": "KKhEzA4C9hzJ"
      },
      "id": "KKhEzA4C9hzJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. CONFIGURATION & NAMING STANDARDS\n",
        "# ==========================================\n",
        "PROJECT_ID = \"project-nirvana-405904\"  # <--- REPLACE THIS\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "# Versions\n",
        "PROJECT_TAG = \"csv\"\n",
        "SOURCE_VERSION = \"001\" # <--- CHANGE THIS TO THE CURRENT Transcripts Validator TABLE VERSION\n",
        "DATA_VERSION = \"005\" # <--- CHANGE THIS TO THE CURRENT Synthetic Data TABLE VERSION\n",
        "SCRIPT_VERSION = \"003\"\n",
        "DESTINATION_TABLE_VERSION = \"003\"\n",
        "\n",
        "# Resources\n",
        "DATASET_ID = f\"vel_{PROJECT_TAG}_schema\"\n",
        "TRANSCRIPTION_TABLE = f\"vel_{PROJECT_TAG}_synthetic_transcripts_{DATA_VERSION}\"\n",
        "SOURCE_TABLE = f\"vel_{PROJECT_TAG}_transcripts_validation_{SOURCE_VERSION}\"\n",
        "DESTINATION_TABLE = f\"vel_{PROJECT_TAG}_derived_signals_{DESTINATION_TABLE_VERSION}\"\n",
        "\n",
        "# Initialize Vertex AI and BigQuery\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "model = GenerativeModel(\"gemini-2.5-flash\")"
      ],
      "metadata": {
        "id": "t2B_Pe0Cjqzz"
      },
      "id": "t2B_Pe0Cjqzz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. The system prompt"
      ],
      "metadata": {
        "id": "NRoEnd37jkX3"
      },
      "id": "NRoEnd37jkX3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prompt configures the LLM to act as a \"Senior Analyst.\"\n",
        "\n",
        "It performs three critical tasks on the raw transcripts:\n",
        "\n",
        "1. Mining \"Organic Signals\": Detecting unstructured risks/opportunities (e.g., \"Creator is burnout\").\n",
        "\n",
        "2. Signal Classification: Actionability for Youtube or out of their control.\n",
        "\n",
        "3. Scoring SPM Effectiveness: Grading the manager's performance (0-100) based on resolution and empathy.\n",
        "\n",
        "Output is forced into strict JSON for direct database insertion."
      ],
      "metadata": {
        "id": "tLhxhavgsQ1k"
      },
      "id": "tLhxhavgsQ1k"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. THE SYSTEM PROMPT\n",
        "# ==========================================\n",
        "PROMPT_SIGNAL_EXTRACTOR = \"\"\"\n",
        "You are a Senior Data and Behavior Analyst at YouTube, specializing in the \"Creator Economy.\" Your job is to audit call transcripts between a Strategic Partner Manager (SPM) and a Content Creator.\n",
        "\n",
        "You have three primary objectives:\n",
        "\n",
        "1. ORGANIC SIGNAL DETECTION:\n",
        "   - Identify critical themes regarding the Creator's status, risk, or opportunity.\n",
        "   - Examples: \"Churn Risk\", \"Copyright Confusion\", \"Interest in Shopping\", \"Burnout\", \"Payment Issues\".\n",
        "\n",
        "2. SIGNAL CLASSIFICATION (ACTIONABILITY):\n",
        "   - For each signal, determine if it is **\"Actionable\"** or **\"Non-Actionable\"** for YouTube.\n",
        "   - **Actionable:** YouTube can do something about it. (e.g., fix a bug, clarify a policy, escalate a support ticket, offer a beta feature, provide training).\n",
        "   - **Non-Actionable:** Issues out of YouTube's control. (e.g., Creator's personal fatigue unrelated to platform, market competition generic trends, weather, personal life).\n",
        "\n",
        "3. SPM SCORING:\n",
        "   - Rate the SPM's effectiveness (1-100) based on Empathy, Clarity, and Resolution.\n",
        "\n",
        "OUTPUT FORMAT (STRICT JSON):\n",
        "Respond ONLY with a valid JSON object:\n",
        "{\n",
        "  \"spm_score\": [0-100],\n",
        "  \"spm_reasoning\": \"Explanation...\",\n",
        "  \"signals\": [\n",
        "    {\n",
        "      \"signal_name\": \"Short Name (e.g., Monetization Bug)\",\n",
        "      \"category\": \"Risk | Opportunity | Product_Feedback | Operational\",\n",
        "      \"actionability\": \"Actionable | Non-Actionable\",\n",
        "      \"description\": \"Brief description of the issue\",\n",
        "      \"evidence_quote\": \"Verbatim quote from the creator\"\n",
        "    }\n",
        "  ],\n",
        "  \"recommended_next_step\": \"Suggested action...\"\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yNvCEOrwjkKW"
      },
      "id": "yNvCEOrwjkKW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Data ingestion (read from bq)"
      ],
      "metadata": {
        "id": "HMgY8Aj_jW3V"
      },
      "id": "HMgY8Aj_jW3V"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. DATA INGESTION (READ FROM BQ)\n",
        "# ==========================================\n",
        "def get_transcripts_from_bq(limit=50):\n",
        "    \"\"\"\n",
        "    Fetches transcripts from BigQuery that have not been processed yet.\n",
        "    Handles cases where the destination table might not exist during the first run.\n",
        "    \"\"\"\n",
        "\n",
        "    # Reference for the destination table to check for existing records\n",
        "    table_id = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
        "\n",
        "    # SQL snippet to exclude already processed IDs\n",
        "    filter_already_processed = f\"\"\"\n",
        "        AND v.conversation_id NOT IN (\n",
        "            SELECT DISTINCT transcript_id FROM `{table_id}`\n",
        "        )\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if the destination table exists to avoid \"Table not found\" errors in the query\n",
        "    try:\n",
        "        client.get_table(table_id)\n",
        "        print(f\"ðŸ” Destination table detected. Filtering out already processed records...\")\n",
        "    except NotFound:\n",
        "        # If the table is not found, we assume it's the first run and skip the filter\n",
        "        print(f\"â„¹ï¸ Destination table does not exist yet. Processing all available records.\")\n",
        "        filter_already_processed = \"\"\n",
        "\n",
        "    # Main query to fetch raw transcripts and metadata\n",
        "    query = f\"\"\"\n",
        "        SELECT\n",
        "            v.conversation_id as transcript_id,\n",
        "            v.creator_id,\n",
        "            v.spm_name,\n",
        "            v.creator_region,\n",
        "            d.raw_transcript as transcript\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}` v\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.{TRANSCRIPTION_TABLE}` d\n",
        "          ON v.conversation_id = d.conversation_id\n",
        "        WHERE v.is_valid\n",
        "        {filter_already_processed}\n",
        "        LIMIT {limit}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        print(f\"ðŸ“¥ Fetching data from BigQuery...\")\n",
        "        df = client.query(query).to_dataframe()\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error reading from BigQuery: {e}\")\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "h20crvsmjXIY"
      },
      "id": "h20crvsmjXIY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Analysis Engine (LLM processing)"
      ],
      "metadata": {
        "id": "APHvHbMxi5Eo"
      },
      "id": "APHvHbMxi5Eo"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. ANALYSIS ENGINE (LLM PROCESSING)\n",
        "# ==========================================\n",
        "def analyze_transcripts(df):\n",
        "    results = []\n",
        "    print(f\"ðŸ§  Analyzing {len(df)} rows with Gemini...\")\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        try:\n",
        "            user_prompt = f\"\"\"\n",
        "            METADATA:\n",
        "            - TRANSCRIPT_ID: {row['transcript_id']}\n",
        "            - SPM NAME: {row['spm_name']}\n",
        "            - CREATOR ID: {row['creator_id']}\n",
        "            - CREATOR REGION: {row['creator_region']}\n",
        "\n",
        "            TRANSCRIPT TEXT:\n",
        "            {row['transcript']}\n",
        "            \"\"\"\n",
        "\n",
        "            response = model.generate_content(\n",
        "                [PROMPT_SIGNAL_EXTRACTOR, user_prompt],\n",
        "                generation_config=GenerationConfig(\n",
        "                    temperature=0.0,\n",
        "                    response_mime_type=\"application/json\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "            analysis = json.loads(response.text)\n",
        "\n",
        "            for signal in analysis.get('signals', []):\n",
        "                results.append({\n",
        "                    \"transcript_id\": row['transcript_id'],\n",
        "                    \"spm_name\": row['spm_name'],\n",
        "                    \"creator_id\": row['creator_id'],\n",
        "                    \"creator_region\": row['creator_region'],\n",
        "                    \"spm_score\": analysis.get('spm_score'),\n",
        "                    \"spm_reasoning\": analysis.get('spm_reasoning'),\n",
        "\n",
        "                    # --- SIGNAL FIELDS ---\n",
        "                    \"signal_name\": signal.get('signal_name'),\n",
        "                    \"signal_category\": signal.get('category'),\n",
        "                    \"signal_actionability\": signal.get('actionability'),\n",
        "                    \"signal_description\": signal.get('description'),\n",
        "                    \"signal_evidence\": signal.get('evidence_quote', signal.get('evidence')),\n",
        "\n",
        "                    \"recommended_action\": analysis.get('recommended_next_step')\n",
        "                })\n",
        "\n",
        "            print(f\"  -> Processed {row['transcript_id']} | Signals found: {len(analysis.get('signals', []))}\")\n",
        "            print(user_prompt)\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error analyzing {row['transcript_id']}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "PhJSmJ_xi4jD"
      },
      "id": "PhJSmJ_xi4jD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Upload Results to BigQuery"
      ],
      "metadata": {
        "id": "QCC_xkPCih1N"
      },
      "id": "QCC_xkPCih1N"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. UPLOAD RESULTS TO BIGQUERY\n",
        "# ==========================================\n",
        "def initialize_destination_table():\n",
        "    \"\"\"\n",
        "    Creates the destination table if it doesn't exist yet to prevent JOIN errors.\n",
        "    \"\"\"\n",
        "    table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
        "\n",
        "    # Define the strict schema here\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"transcript_id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"spm_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"creator_id\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"creator_region\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"spm_score\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"spm_reasoning\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"signal_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"signal_category\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"signal_actionability\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"signal_description\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"signal_evidence\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"recommended_action\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"processed_at\", \"TIMESTAMP\")\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        client.get_table(table_ref)\n",
        "        print(f\"âœ… Destination table {DESTINATION_TABLE} exists.\")\n",
        "    except NotFound:\n",
        "        print(f\"âš ï¸ Destination table not found. Creating {DESTINATION_TABLE}...\")\n",
        "        table = bigquery.Table(table_ref, schema=schema)\n",
        "        client.create_table(table)\n",
        "        print(\"âœ… Table created successfully.\")\n",
        "\n",
        "\n",
        "def upload_signals_to_bq(df):\n",
        "    if df.empty:\n",
        "        print(\"âš ï¸ No signals detected. Nothing to upload.\")\n",
        "        return\n",
        "\n",
        "    # Add timestamp\n",
        "    df['processed_at'] = pd.Timestamp.now()\n",
        "\n",
        "    table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        write_disposition=\"WRITE_APPEND\",\n",
        "        # Use the exact same schema defined in the init function\n",
        "        schema=[\n",
        "            bigquery.SchemaField(\"transcript_id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "            bigquery.SchemaField(\"spm_name\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"creator_id\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"creator_region\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"spm_score\", \"INTEGER\"),\n",
        "            bigquery.SchemaField(\"spm_reasoning\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"signal_name\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"signal_category\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"signal_actionability\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"signal_description\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"signal_evidence\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"recommended_action\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"processed_at\", \"TIMESTAMP\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(f\"ðŸš€ Uploading {len(df)} detected signals to {table_ref}...\")\n",
        "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
        "    job.result()\n",
        "    print(\"âœ… Upload Complete!\")"
      ],
      "metadata": {
        "id": "0Zl4T2XPik11"
      },
      "id": "0Zl4T2XPik11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Main"
      ],
      "metadata": {
        "id": "e4HKuRwCNt3c"
      },
      "id": "e4HKuRwCNt3c"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    # 1. Initialize Destination Table (Crucial Step)\n",
        "    initialize_destination_table()\n",
        "\n",
        "    # 2. Read transcriptions\n",
        "    df_transcripts = get_transcripts_from_bq(limit=120)\n",
        "    end_time_read = time.perf_counter()\n",
        "\n",
        "    # 3. Analyze with LLM\n",
        "    df_signals = analyze_transcripts(df_transcripts)\n",
        "    end_time_llm = time.perf_counter()\n",
        "\n",
        "    # 4. Save Results\n",
        "    upload_signals_to_bq(df_signals)\n",
        "    end_time = time.perf_counter()\n",
        "\n",
        "    # Execution times\n",
        "    duration_read = str(timedelta(seconds=end_time_read - start_time))\n",
        "    duration_llm = str(timedelta(seconds=end_time_llm - end_time_read))\n",
        "    duration_save = str(timedelta(seconds=end_time - end_time_llm))\n",
        "    duration_total = str(timedelta(seconds=end_time - start_time))\n",
        "\n",
        "    print(f\"Read BigQuery execution time (HH:MM:SS): {duration_read}\")\n",
        "    print(f\"LLM Analysis execution time (HH:MM:SS): {duration_llm}\")\n",
        "    print(f\"Save signals to BigQuery execution time (HH:MM:SS): {duration_save}\")\n",
        "    print(f\"Total execution time (HH:MM:SS): {duration_total}\")\n",
        "\n",
        "    # Preview\n",
        "    print(\"\\n--- PREVIEW OF DETECTED SIGNALS ---\")\n",
        "    try:\n",
        "      print(df_signals[['spm_name', 'signal_name', 'spm_score']].head().to_markdown(index=False))\n",
        "    except:\n",
        "      print(df_signals.head().to_markdown(index=False))"
      ],
      "metadata": {
        "id": "RZtiauz0Nwwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a5bd09-ba5b-42db-ddf9-9381f8e4a43d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1771542522900,
          "user_tz": 360,
          "elapsed": 1434626,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "RZtiauz0Nwwg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Destination table not found. Creating vel_csv_derived_signals_003...\n",
            "âœ… Table created successfully.\n",
            "ðŸ” Destination table detected. Filtering out already processed records...\n",
            "ðŸ“¥ Fetching data from BigQuery...\n",
            "ðŸ§  Analyzing 73 rows with Gemini...\n",
            "  -> Processed 1_0_1771463705 | Signals found: 5\n",
            "  -> Processed 8_1_1771464210 | Signals found: 6\n",
            "  -> Processed 30_0_1771465427 | Signals found: 6\n",
            "  -> Processed 3_0_1771463776 | Signals found: 13\n",
            "  -> Processed 3_1_1771463831 | Signals found: 12\n",
            "  -> Processed 1_0_1771530949 | Signals found: 7\n",
            "  -> Processed 72_1_1771467932 | Signals found: 7\n",
            "  -> Processed 1_1_1771530986 | Signals found: 12\n",
            "  -> Processed 39_0_1771465931 | Signals found: 12\n",
            "  -> Processed 28_0_1771465305 | Signals found: 14\n",
            "  -> Processed 41_0_1771466063 | Signals found: 11\n",
            "  -> Processed 72_0_1771467872 | Signals found: 7\n",
            "  -> Processed 8_0_1771464161 | Signals found: 14\n",
            "  -> Processed 60_1_1771467116 | Signals found: 10\n",
            "  -> Processed 38_1_1771465902 | Signals found: 10\n",
            "  -> Processed 67_0_1771467463 | Signals found: 13\n",
            "  -> Processed 4_0_1771531118 | Signals found: 12\n",
            "  -> Processed 62_0_1771467201 | Signals found: 7\n",
            "  -> Processed 73_0_1771467969 | Signals found: 13\n",
            "  -> Processed 60_0_1771467085 | Signals found: 10\n",
            "  -> Processed 13_0_1771464461 | Signals found: 14\n",
            "  -> Processed 14_0_1771464510 | Signals found: 7\n",
            "  -> Processed 31_0_1771465461 | Signals found: 11\n",
            "  -> Processed 11_0_1771464368 | Signals found: 9\n",
            "  -> Processed 38_0_1771465867 | Signals found: 12\n",
            "  -> Processed 18_0_1771464705 | Signals found: 10\n",
            "  -> Processed 18_1_1771464746 | Signals found: 13\n",
            "  -> Processed 68_1_1771467565 | Signals found: 7\n",
            "  -> Processed 29_0_1771465357 | Signals found: 7\n",
            "  -> Processed 33_0_1771465549 | Signals found: 10\n",
            "  -> Processed 65_0_1771467375 | Signals found: 9\n",
            "  -> Processed 16_0_1771464574 | Signals found: 14\n",
            "  -> Processed 16_1_1771464623 | Signals found: 10\n",
            "  -> Processed 21_0_1771464914 | Signals found: 10\n",
            "  -> Processed 23_0_1771465054 | Signals found: 8\n",
            "  -> Processed 2_0_1771531023 | Signals found: 14\n",
            "  -> Processed 7_1_1771531308 | Signals found: 11\n",
            "  -> Processed 3_0_1771531075 | Signals found: 12\n",
            "  -> Processed 21_1_1771464950 | Signals found: 13\n",
            "  -> Processed 29_1_1771465393 | Signals found: 7\n",
            "  -> Processed 40_0_1771466017 | Signals found: 7\n",
            "  -> Processed 20_1_1771464875 | Signals found: 7\n",
            "  -> Processed 55_0_1771466806 | Signals found: 10\n",
            "  -> Processed 55_1_1771466860 | Signals found: 12\n",
            "  -> Processed 9_1_1771531512 | Signals found: 11\n",
            "  -> Processed 27_1_1771465267 | Signals found: 13\n",
            "  -> Processed 35_0_1771465641 | Signals found: 10\n",
            "  -> Processed 54_0_1771466758 | Signals found: 14\n",
            "  -> Processed 10_0_1771464321 | Signals found: 6\n",
            "  -> Processed 56_0_1771466913 | Signals found: 11\n",
            "  -> Processed 20_0_1771464833 | Signals found: 12\n",
            "  -> Processed 51_0_1771466595 | Signals found: 14\n",
            "  -> Processed 2_0_1771463740 | Signals found: 9\n",
            "  -> Processed 46_0_1771466304 | Signals found: 15\n",
            "  -> Processed 35_1_1771465687 | Signals found: 7\n",
            "  -> Processed 58_0_1771466993 | Signals found: 7\n",
            "  -> Processed 15_0_1771464542 | Signals found: 9\n",
            "  -> Processed 6_0_1771531227 | Signals found: 8\n",
            "  -> Processed 17_0_1771464669 | Signals found: 8\n",
            "  -> Processed 45_0_1771466262 | Signals found: 9\n",
            "  -> Processed 4_0_1771463882 | Signals found: 18\n",
            "  -> Processed 42_0_1771466105 | Signals found: 15\n",
            "  -> Processed 70_1_1771467719 | Signals found: 10\n",
            "  -> Processed 71_0_1771467758 | Signals found: 14\n",
            "  -> Processed 7_1_1771464113 | Signals found: 10\n",
            "  -> Processed 50_1_1771466541 | Signals found: 11\n",
            "  -> Processed 71_1_1771467818 | Signals found: 13\n",
            "  -> Processed 50_0_1771466500 | Signals found: 8\n",
            "  -> Processed 9_0_1771464246 | Signals found: 7\n",
            "âŒ Error analyzing 9_1_1771464285: Expecting ',' delimiter: line 3 column 624 (char 644)\n",
            "  -> Processed 10_0_1771531556 | Signals found: 11\n",
            "  -> Processed 15_0_1771531852 | Signals found: 15\n",
            "  -> Processed 36_0_1771465719 | Signals found: 6\n",
            "ðŸš€ Uploading 746 detected signals to project-nirvana-405904.vel_csv_schema.vel_csv_derived_signals_003...\n",
            "âœ… Upload Complete!\n",
            "Read BigQuery execution time (HH:MM:SS): 0:00:03.482719\n",
            "LLM Analysis execution time (HH:MM:SS): 0:23:47.986211\n",
            "Save signals to BigQuery execution time (HH:MM:SS): 0:00:03.069748\n",
            "Total execution time (HH:MM:SS): 0:23:54.538677\n",
            "\n",
            "--- PREVIEW OF DETECTED SIGNALS ---\n",
            "| spm_name    | signal_name                                          |   spm_score |\n",
            "|:------------|:-----------------------------------------------------|------------:|\n",
            "| Noah Rhodes | Notification Delivery Issue                          |          92 |\n",
            "| Noah Rhodes | Monetization Metrics Confusion (RPM/CPM)             |          92 |\n",
            "| Noah Rhodes | Underutilization of Channel Memberships              |          92 |\n",
            "| Noah Rhodes | Unawareness of YouTube Shopping Feature              |          92 |\n",
            "| Noah Rhodes | Difficulty Interpreting Audience Retention Analytics |          92 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In average it takes ~20 seconds to extract signals from a transcription"
      ],
      "metadata": {
        "id": "etp4JSZPG4qZ"
      },
      "id": "etp4JSZPG4qZ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "vel_csv_signals_extractor_003",
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
