{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Main Code"
      ],
      "metadata": {
        "id": "bjU7yc3ANkH_"
      },
      "id": "bjU7yc3ANkH_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Libraries"
      ],
      "metadata": {
        "id": "-nOP2Av89eWT"
      },
      "id": "-nOP2Av89eWT"
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import NotFound\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import timedelta"
      ],
      "metadata": {
        "id": "edbUQVlo9cqc"
      },
      "id": "edbUQVlo9cqc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Configuration & Naming Standards"
      ],
      "metadata": {
        "id": "KKhEzA4C9hzJ"
      },
      "id": "KKhEzA4C9hzJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. CONFIGURATION & NAMING STANDARDS\n",
        "# ==========================================\n",
        "PROJECT_ID = \"project-nirvana-405904\"  # <--- REPLACE THIS\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "# Versions\n",
        "PROJECT_TAG = \"csv\"\n",
        "DATA_VERSION = \"006\" # Current Synthetic Data TABLE VERSION\n",
        "SCRIPT_VERSION = \"005\"\n",
        "DESTINATION_TABLE_VERSION = \"005\"\n",
        "\n",
        "# Resources\n",
        "DATASET_ID = f\"vel_{PROJECT_TAG}_schema\"\n",
        "# The validation table is deprecated; we now read directly from the unified transcript table\n",
        "TRANSCRIPTION_TABLE = f\"vel_{PROJECT_TAG}_synthetic_transcripts_{DATA_VERSION}\"\n",
        "DESTINATION_TABLE = f\"vel_{PROJECT_TAG}_derived_signals_{DESTINATION_TABLE_VERSION}\"\n",
        "\n",
        "# Initialize Vertex AI and BigQuery\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "model = GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "# TAXONOMY\n",
        "SIGNAL_TAXONOMY_LIST = [\n",
        "    # Monetization\n",
        "    \"[Monetization > Ad Revenue Optimization] RPM\",\n",
        "    \"[Monetization > Ad Revenue Optimization] CPM\",\n",
        "    \"[Monetization > Ad Revenue Optimization] Geo Mix\",\n",
        "    \"[Monetization > Ad Revenue Optimization] Seasonality\",\n",
        "    \"[Monetization > Ad Revenue Optimization] Long Form vs Shorts\",\n",
        "    \"[Monetization > Fan Funding Optimization] Channel Memberships\",\n",
        "    \"[Monetization > Fan Funding Optimization] Super Chats\",\n",
        "    \"[Monetization > Fan Funding Optimization] Super Thanks\",\n",
        "    \"[Monetization > Fan Funding Optimization] Recurring Revenue\",\n",
        "    \"[Monetization > Fan Funding Optimization] Churn\",\n",
        "    \"[Monetization > Commerce Optimization] Shopping\",\n",
        "    \"[Monetization > Commerce Optimization] Affiliate\",\n",
        "    \"[Monetization > Commerce Optimization] Product Tagging\",\n",
        "    \"[Monetization > Commerce Optimization] Conversion\",\n",
        "    \"[Monetization > Brand Revenue Strategy] Brand Deals\",\n",
        "    \"[Monetization > Brand Revenue Strategy] Brand Connect\",\n",
        "    \"[Monetization > Brand Revenue Strategy] Sponsor Integrations\",\n",
        "    \"[Monetization > Brand Revenue Strategy] Platform Ads\",\n",
        "    # Content & Formats\n",
        "    \"[Content & Formats > Shorts Strategy] Shorts growth vs revenue\",\n",
        "    \"[Content & Formats > Shorts Strategy] Shorts Collab\",\n",
        "    \"[Content & Formats > Shorts Strategy] Shorts Experimentation\",\n",
        "    \"[Content & Formats > Live and Event Strategy] Live Streaming\",\n",
        "    \"[Content & Formats > Live and Event Strategy] Premieres\",\n",
        "    \"[Content & Formats > Live and Event Strategy] Redirect Strategy\",\n",
        "    \"[Content & Formats > Live and Event Strategy] Real-Time Monetization\",\n",
        "    \"[Content & Formats > Live and Event Strategy] Scheduled Launches\",\n",
        "    \"[Content & Formats > Content Packaging] Titles, Thumbnails\",\n",
        "    \"[Content & Formats > Content Packaging] Hooks\",\n",
        "    \"[Content & Formats > Content Packaging] Chapters\",\n",
        "    \"[Content & Formats > Content Packaging] Video Structure Strategy\",\n",
        "    # Tools and Policy\n",
        "    \"[Tools and Policy > Copyright and content] Claims vs Strikes\",\n",
        "    \"[Tools and Policy > Copyright and content] Disputes\",\n",
        "    \"[Tools and Policy > Copyright and content] Copyright Risk Management\",\n",
        "    \"[Tools and Policy > Brand Safety and Ads] Yellow Icon\",\n",
        "    \"[Tools and Policy > Brand Safety and Ads] Advertiser Suitability\",\n",
        "    \"[Tools and Policy > Brand Safety and Ads] Self Certification\",\n",
        "    \"[Tools and Policy > Brand Safety and Ads] Ad Restrictions\",\n",
        "    # Creator Health and Ops\n",
        "    \"[Creator Health and Ops > Sustainability and Ops] Burnout\",\n",
        "    \"[Creator Health and Ops > Sustainability and Ops] Upload Cadence Stress\",\n",
        "    \"[Creator Health and Ops > Sustainability and Ops] Team Scaling\",\n",
        "    \"[Creator Health and Ops > Sustainability and Ops] Prod Workflow Strain\",\n",
        "    # Relationship and Strategic Support\n",
        "    \"[Relationship and Strategic Support > Strategic Partnership and Support] Feedback on SPM support\",\n",
        "    \"[Relationship and Strategic Support > Strategic Partnership and Support] Need for Escalation\",\n",
        "    \"[Relationship and Strategic Support > Strategic Partnership and Support] Milestone Logistics\",\n",
        "    \"[Relationship and Strategic Support > Strategic Partnership and Support] Awards/Events\",\n",
        "    # Analytics and Growth\n",
        "    \"[Analytics and Growth > Retention and Discovery] Audience Retention Curves\",\n",
        "    \"[Analytics and Growth > Retention and Discovery] Returning Viewers\",\n",
        "    \"[Analytics and Growth > Retention and Discovery] Engagement Depth\",\n",
        "    \"[Analytics and Growth > Retention and Discovery] Subscriber Conversion\",\n",
        "    \"[Analytics and Growth > Traffic and Discovery] Browse vs Search vs Suggested\",\n",
        "    \"[Analytics and Growth > Traffic and Discovery] Growth Volatility\",\n",
        "    \"[Analytics and Growth > Traffic and Discovery] Traffic Source Dependency\",\n",
        "    \"[Analytics and Growth > Topic and Demand Discovery] Research Tab\",\n",
        "    \"[Analytics and Growth > Topic and Demand Discovery] Keyword Demand\",\n",
        "    \"[Analytics and Growth > Topic and Demand Discovery] Content Ideation off Audience\",\n",
        "    \"[Analytics and Growth > Topic and Demand Discovery] Search Trends Buff\",\n",
        "    \"[Analytics and Growth > Performance Metric Interpretation] Confusion/Discussion on RPM vs CPM\",\n",
        "    \"[Analytics and Growth > Performance Metric Interpretation] Impression\",\n",
        "    \"[Analytics and Growth > Performance Metric Interpretation] CTR\",\n",
        "    \"[Analytics and Growth > Performance Metric Interpretation] Analytics Insights to Gaps\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "t2B_Pe0Cjqzz"
      },
      "id": "t2B_Pe0Cjqzz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. The system prompt"
      ],
      "metadata": {
        "id": "NRoEnd37jkX3"
      },
      "id": "NRoEnd37jkX3"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. THE SYSTEM PROMPT\n",
        "# ==========================================\n",
        "PROMPT_SIGNAL_EXTRACTOR = f\"\"\"\n",
        "You are a Senior Data and Behavior Analyst at YouTube. Your job is to audit call transcripts between a Strategic Partner Manager (SPM) and a Content Creator.\n",
        "Do not assume anything about the creator's mood or channel persona; deduce everything purely from the dialogue.\n",
        "\n",
        "You have three primary objectives:\n",
        "\n",
        "1. ORGANIC SIGNAL DETECTION (STRICT TAXONOMY):\n",
        "   - You MUST extract signals based ONLY on the following predefined list. Each item is formatted as \"[Category > Sub-Category] Topic\".\n",
        "   {json.dumps(SIGNAL_TAXONOMY_LIST, indent=2)}\n",
        "   - Ignore any topic discussed that does not map perfectly to one of these Specific Topics.\n",
        "\n",
        "2. SIGNAL CLASSIFICATION & SENTIMENT:\n",
        "   - **Actionability:** Determine if the issue is \"Actionable\" (YouTube/SPM can intervene) or \"Non-Actionable\".\n",
        "   - **Sentiment:** Classify the creator's tone regarding the specific topic as \"Positive\", \"Negative\", or \"Neutral\".\n",
        "\n",
        "3. SPM SCORING:\n",
        "   - Rate the SPM's effectiveness (1-100) based on Empathy, Clarity, and Resolution.\n",
        "   - **Agenda Adherence Penalty:** Compare the topics the SPM stated they would discuss against what was ACTUALLY covered. Heavily penalize the score if the SPM claims to have discussed topics that were not covered (e.g., stated 5 topics but only covered 2).\n",
        "\n",
        "4. CRITICAL JSON FORMATTING RULES:\n",
        "   - NEVER use double quotes (\"\") inside any of your text values.\n",
        "   - If you need to quote the creator or emphasize a word, you MUST use single quotes ('') instead to avoid breaking the JSON structure.\n",
        "   - Do not include newline characters (\\n) inside your text values.\n",
        "\n",
        "OUTPUT FORMAT (STRICT JSON):\n",
        "Respond ONLY with a valid JSON object. Extract the Category, Sub-Category, and Topic from the brackets in the list.\n",
        "{{\n",
        "  \"spm_score\": [0-100],\n",
        "  \"spm_reasoning\": \"Explanation of score including Empathy and Agenda Adherence.\",\n",
        "  \"signals\": [\n",
        "    {{\n",
        "      \"signal_category\": \"Exact Category from the brackets\",\n",
        "      \"signal_sub_category\": \"Exact Sub-Category from the brackets\",\n",
        "      \"signal_name\": \"Exact Topic Name outside the brackets\",\n",
        "      \"signal_sentiment\": \"Positive | Negative | Neutral\",\n",
        "      \"signal_actionability\": \"Actionable | Non-Actionable\",\n",
        "      \"signal_description\": \"Brief description of the issue\",\n",
        "      \"signal_evidence\": \"Verbatim quote from the creator\"\n",
        "    }}\n",
        "  ],\n",
        "  \"recommended_next_step\": \"Suggested action...\"\n",
        "}}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yNvCEOrwjkKW"
      },
      "id": "yNvCEOrwjkKW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Data ingestion (read from bq)"
      ],
      "metadata": {
        "id": "HMgY8Aj_jW3V"
      },
      "id": "HMgY8Aj_jW3V"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. DATA INGESTION (READ FROM BQ)\n",
        "# ==========================================\n",
        "def get_transcripts_from_bq(limit=50):\n",
        "    \"\"\"\n",
        "    Fetches transcripts from BigQuery preventing data leakage.\n",
        "    Only permitted columns are queried. Duplicates are filtered out.\n",
        "    \"\"\"\n",
        "    table_id = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
        "\n",
        "    filter_already_processed = f\"\"\"\n",
        "        AND conversation_id NOT IN (\n",
        "            SELECT DISTINCT conversation_id FROM `{table_id}`\n",
        "        )\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        client.get_table(table_id)\n",
        "        print(f\"üîç Destination table detected. Filtering out already processed records...\")\n",
        "    except NotFound:\n",
        "        print(f\"‚ÑπÔ∏è Destination table does not exist yet. Processing all available records.\")\n",
        "        filter_already_processed = \"\"\n",
        "\n",
        "    # No JOIN needed anymore. Reading only allowed columns to prevent Data Leakage.\n",
        "    query = f\"\"\"\n",
        "        SELECT\n",
        "            conversation_id,\n",
        "            channel_name,\n",
        "            creator_niche,\n",
        "            creator_region,\n",
        "            spm_name,\n",
        "            duration_minutes,\n",
        "            raw_transcript\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.{TRANSCRIPTION_TABLE}`\n",
        "        WHERE is_valid = True\n",
        "        {filter_already_processed}\n",
        "        LIMIT {limit}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        print(f\"üì• Fetching data from BigQuery...\")\n",
        "        df = client.query(query).to_dataframe()\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading from BigQuery: {e}\")\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "h20crvsmjXIY"
      },
      "id": "h20crvsmjXIY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Analysis Engine (LLM processing)"
      ],
      "metadata": {
        "id": "APHvHbMxi5Eo"
      },
      "id": "APHvHbMxi5Eo"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. ANALYSIS ENGINE (LLM PROCESSING)\n",
        "# ==========================================\n",
        "def analyze_transcripts(df):\n",
        "    results = []\n",
        "    if df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(f\"üß† Analyzing {len(df)} rows with Gemini (Sequential)...\")\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        try:\n",
        "            user_prompt = f\"\"\"\n",
        "            METADATA:\n",
        "            - CONVERSATION_ID: {row['conversation_id']}\n",
        "            - CHANNEL NAME: {row['channel_name']}\n",
        "            - CREATOR NICHE: {row['creator_niche']}\n",
        "            - CREATOR REGION: {row['creator_region']}\n",
        "            - SPM NAME: {row['spm_name']}\n",
        "            - DURATION (MIN): {row['duration_minutes']}\n",
        "\n",
        "            TRANSCRIPT TEXT:\n",
        "            {row['raw_transcript']}\n",
        "            \"\"\"\n",
        "\n",
        "            # --- API Call ---\n",
        "            response = model.generate_content(\n",
        "                [PROMPT_SIGNAL_EXTRACTOR, user_prompt],\n",
        "                generation_config=GenerationConfig(\n",
        "                    temperature=0.0,\n",
        "                    response_mime_type=\"application/json\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # --- Clean up raw text before parsing ---\n",
        "            raw_text = response.text.strip()\n",
        "\n",
        "            # Remove Markdown code blocks if the LLM hallucinated them\n",
        "            if raw_text.startswith(\"```json\"):\n",
        "                raw_text = raw_text[7:]\n",
        "            if raw_text.startswith(\"```\"):\n",
        "                raw_text = raw_text[3:]\n",
        "            if raw_text.endswith(\"```\"):\n",
        "                raw_text = raw_text[:-3]\n",
        "\n",
        "            raw_text = raw_text.strip()\n",
        "\n",
        "            # Parse the JSON string returned by Gemini\n",
        "            analysis = json.loads(response.text)\n",
        "            signals = analysis.get('signals', [])\n",
        "\n",
        "            # --- NEW LOGIC: Handle cases where no signals were detected ---\n",
        "            if not signals:\n",
        "                results.append({\n",
        "                    \"conversation_id\": row['conversation_id'],\n",
        "                    \"channel_name\": row['channel_name'],\n",
        "                    \"creator_niche\": row['creator_niche'],\n",
        "                    \"creator_region\": row['creator_region'],\n",
        "                    \"spm_name\": row['spm_name'],\n",
        "                    \"duration_minutes\": float(row['duration_minutes']) if pd.notnull(row['duration_minutes']) else None,\n",
        "\n",
        "                    # Keep the SPM evaluation data intact\n",
        "                    \"spm_score\": analysis.get('spm_score'),\n",
        "                    \"spm_reasoning\": analysis.get('spm_reasoning'),\n",
        "\n",
        "                    # Explicitly set signal mapping fields to None (NULL in BigQuery)\n",
        "                    \"signal_category\": None,\n",
        "                    \"signal_sub_category\": None,\n",
        "                    \"signal_name\": None,\n",
        "                    \"signal_sentiment\": None,\n",
        "                    \"signal_actionability\": None,\n",
        "                    \"signal_description\": \"No organic signals detected matching the strict taxonomy.\",\n",
        "                    \"signal_evidence\": None,\n",
        "\n",
        "                    \"recommended_action\": analysis.get('recommended_next_step')\n",
        "                })\n",
        "\n",
        "            # --- ORIGINAL LOGIC: If signals exist, create one row per signal ---\n",
        "            else:\n",
        "                for signal in signals:\n",
        "                    results.append({\n",
        "                        \"conversation_id\": row['conversation_id'],\n",
        "                        \"channel_name\": row['channel_name'],\n",
        "                        \"creator_niche\": row['creator_niche'],\n",
        "                        \"creator_region\": row['creator_region'],\n",
        "                        \"spm_name\": row['spm_name'],\n",
        "                        \"duration_minutes\": float(row['duration_minutes']) if pd.notnull(row['duration_minutes']) else None,\n",
        "\n",
        "                        \"spm_score\": analysis.get('spm_score'),\n",
        "                        \"spm_reasoning\": analysis.get('spm_reasoning'),\n",
        "\n",
        "                        # Map the detected signal fields\n",
        "                        \"signal_category\": signal.get('signal_category'),\n",
        "                        \"signal_sub_category\": signal.get('signal_sub_category'),\n",
        "                        \"signal_name\": signal.get('signal_name'),\n",
        "                        \"signal_sentiment\": signal.get('signal_sentiment'),\n",
        "                        \"signal_actionability\": signal.get('signal_actionability'),\n",
        "                        \"signal_description\": signal.get('signal_description'),\n",
        "                        \"signal_evidence\": signal.get('signal_evidence'),\n",
        "\n",
        "                        \"recommended_action\": analysis.get('recommended_next_step')\n",
        "                    })\n",
        "\n",
        "            print(f\"  -> Processed {row['conversation_id']} | Signals found: {len(analysis.get('signals', []))}\")\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error analyzing {row['conversation_id']}: {e}\")\n",
        "\n",
        "            # To check the error\n",
        "            if 'response' in locals():\n",
        "                print(f\"Texto crudo devuelto por Gemini:\\n{response.text}\\n\")\n",
        "\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "PhJSmJ_xi4jD"
      },
      "id": "PhJSmJ_xi4jD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Upload Results to BigQuery"
      ],
      "metadata": {
        "id": "QCC_xkPCih1N"
      },
      "id": "QCC_xkPCih1N"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. UPLOAD RESULTS TO BIGQUERY\n",
        "# ==========================================\n",
        "def initialize_destination_table():\n",
        "    \"\"\"\n",
        "    Creates the destination table using a flat schema optimized for LLM/MCP SQL agents.\n",
        "    \"\"\"\n",
        "    table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
        "\n",
        "    schema = [\n",
        "        # Call Metadata\n",
        "        bigquery.SchemaField(\"conversation_id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"channel_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"creator_niche\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"creator_region\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"spm_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"duration_minutes\", \"FLOAT\"),\n",
        "\n",
        "        # SPM Evaluation\n",
        "        bigquery.SchemaField(\"spm_score\", \"INTEGER\"),\n",
        "        bigquery.SchemaField(\"spm_reasoning\", \"STRING\"),\n",
        "\n",
        "        # Signal Extraction (Categorical limits for easy SQL GROUP BY)\n",
        "        bigquery.SchemaField(\"signal_category\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"signal_sub_category\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"signal_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"signal_sentiment\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"signal_actionability\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"signal_description\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"signal_evidence\", \"STRING\"),\n",
        "\n",
        "        # Next Steps & Audit\n",
        "        bigquery.SchemaField(\"recommended_action\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"processed_at\", \"TIMESTAMP\")\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        client.get_table(table_ref)\n",
        "        print(f\"‚úÖ Destination table {DESTINATION_TABLE} exists.\")\n",
        "    except NotFound:\n",
        "        print(f\"‚ö†Ô∏è Destination table not found. Creating {DESTINATION_TABLE}...\")\n",
        "        table = bigquery.Table(table_ref, schema=schema)\n",
        "        client.create_table(table)\n",
        "        print(\"‚úÖ Table created successfully.\")\n",
        "\n",
        "def upload_signals_to_bq(df):\n",
        "    if df.empty:\n",
        "        print(\"‚ö†Ô∏è No signals detected. Nothing to upload.\")\n",
        "        return\n",
        "\n",
        "    # Add processing timestamp\n",
        "    df['processed_at'] = pd.Timestamp.now()\n",
        "\n",
        "    table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        write_disposition=\"WRITE_APPEND\",\n",
        "        # Use the exact same schema defined in the init function\n",
        "        schema=[\n",
        "            bigquery.SchemaField(\"conversation_id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "            bigquery.SchemaField(\"channel_name\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"creator_niche\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"creator_region\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"spm_name\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"duration_minutes\", \"FLOAT\"),\n",
        "            bigquery.SchemaField(\"spm_score\", \"INTEGER\"),\n",
        "            bigquery.SchemaField(\"spm_reasoning\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"signal_category\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"signal_sub_category\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"signal_name\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"signal_sentiment\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"signal_actionability\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"signal_description\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"signal_evidence\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"recommended_action\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"processed_at\", \"TIMESTAMP\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(f\"üöÄ Uploading {len(df)} detected signals to {table_ref}...\")\n",
        "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
        "    job.result()\n",
        "    print(\"‚úÖ Upload Complete!\")"
      ],
      "metadata": {
        "id": "0Zl4T2XPik11"
      },
      "id": "0Zl4T2XPik11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Main"
      ],
      "metadata": {
        "id": "e4HKuRwCNt3c"
      },
      "id": "e4HKuRwCNt3c"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    # 1. Initialize Destination Table (Crucial Step)\n",
        "    initialize_destination_table()\n",
        "\n",
        "    # 2. Read transcriptions (Cleaned to avoid Data Leakage)\n",
        "    df_transcripts = get_transcripts_from_bq(limit=150)\n",
        "    end_time_read = time.perf_counter()\n",
        "\n",
        "    # 3. Analyze with LLM\n",
        "    df_signals = analyze_transcripts(df_transcripts)\n",
        "    end_time_llm = time.perf_counter()\n",
        "\n",
        "    # 4. Save Results\n",
        "    upload_signals_to_bq(df_signals)\n",
        "    end_time = time.perf_counter()\n",
        "\n",
        "    # Execution times\n",
        "    duration_read = str(timedelta(seconds=end_time_read - start_time))\n",
        "    duration_llm = str(timedelta(seconds=end_time_llm - end_time_read))\n",
        "    duration_save = str(timedelta(seconds=end_time - end_time_llm))\n",
        "    duration_total = str(timedelta(seconds=end_time - start_time))\n",
        "\n",
        "    print(f\"Read BigQuery execution time (HH:MM:SS): {duration_read}\")\n",
        "    print(f\"LLM Analysis execution time (HH:MM:SS): {duration_llm}\")\n",
        "    print(f\"Save signals to BigQuery execution time (HH:MM:SS): {duration_save}\")\n",
        "    print(f\"Total execution time (HH:MM:SS): {duration_total}\")\n",
        "\n",
        "    # Preview\n",
        "    print(\"\\n--- PREVIEW OF DETECTED SIGNALS ---\")\n",
        "    try:\n",
        "      print(df_signals[['spm_name', 'signal_name', 'signal_sentiment', 'spm_score']].head().to_markdown(index=False))\n",
        "    except:\n",
        "      print(df_signals.head().to_markdown(index=False))"
      ],
      "metadata": {
        "id": "RZtiauz0Nwwg"
      },
      "id": "RZtiauz0Nwwg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In average it takes:\n",
        "\n",
        "~2 to 4 seconds to extract the data from bigquery\n",
        "\n",
        "~43.5 seconds to extract signals from a transcription\n",
        "\n",
        "~3.5 seconds to upload extracted signals to bigquery"
      ],
      "metadata": {
        "id": "etp4JSZPG4qZ"
      },
      "id": "etp4JSZPG4qZ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "vel_csv_signals_extractor_005_WIP",
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
