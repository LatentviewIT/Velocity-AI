{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Main Code"
      ],
      "metadata": {
        "id": "bjU7yc3ANkH_"
      },
      "id": "bjU7yc3ANkH_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Libraries"
      ],
      "metadata": {
        "id": "-nOP2Av89eWT"
      },
      "id": "-nOP2Av89eWT"
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import GoogleAPIError\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Any, Optional"
      ],
      "metadata": {
        "id": "edbUQVlo9cqc"
      },
      "id": "edbUQVlo9cqc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Configuration & Naming Standards"
      ],
      "metadata": {
        "id": "KKhEzA4C9hzJ"
      },
      "id": "KKhEzA4C9hzJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. CONFIGURATION & NAMING STANDARDS\n",
        "# ==========================================\n",
        "\n",
        "PROJECT_ID = \"project-nirvana-405904\"  # <--- REPLACE THIS\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "# Versions\n",
        "PROJECT_TAG = \"csv\"\n",
        "TRANSCRIPTION_VERSION = \"005\" # <--- CHANGE THIS TO THE CURRENT Synthetic Transcripts TABLE VERSION\n",
        "SOURCE_VERSION = \"003\" # <--- CHANGE THIS TO THE CURRENT Signals Derived TABLE VERSION\n",
        "SCRIPT_VERSION = \"001\"\n",
        "DESTINATION_TABLE_VERSION = \"001\"\n",
        "\n",
        "# Resources\n",
        "DATASET_ID = f\"vel_{PROJECT_TAG}_schema\"\n",
        "SOURCE_TABLE = f\"vel_{PROJECT_TAG}_derived_signals_{SOURCE_VERSION}\"\n",
        "DESTINATION_TABLE = f\"vel_{PROJECT_TAG}_signals_validation_{DESTINATION_TABLE_VERSION}\"\n",
        "TRANSCRIPTS_TABLE = f\"vel_{PROJECT_TAG}_synthetic_transcripts_{TRANSCRIPTION_VERSION}\"\n",
        "\n",
        "# Initialize Vertex AI\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "model = GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "# Initialize BigQuery\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)"
      ],
      "metadata": {
        "id": "3m1YdXlR9iNx"
      },
      "id": "3m1YdXlR9iNx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Validator System Prompt (Strict Auditor)"
      ],
      "metadata": {
        "id": "JZkNBYaT9m8n"
      },
      "id": "JZkNBYaT9m8n"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. VALIDATOR SYSTEM PROMPT (Strict Auditor)\n",
        "# ==========================================\n",
        "\n",
        "VALIDATOR_SYSTEM_PROMPT = \"\"\"\n",
        "You are an automated **Shadow Analyst & Validator**: a forensic auditor for an AI Signal Extractor.\n",
        "Your job is to evaluate if the Extractor correctly analyzed a YouTube Creator vs. SPM transcript AND if it missed any critical business insights.\n",
        "\n",
        "You will receive a JSON payload containing:\n",
        "1. \"original_transcript\": The full raw conversation.\n",
        "2. \"extracted_signals\": An ARRAY of all structured signals generated by the Extractor Agent for this transcript.\n",
        "\n",
        "REQUIREMENTS & EVALUATION PILLARS:\n",
        "1. Verify Existing Signals:\n",
        "   - For EACH signal in the \"extracted_signals\" array, check for Evidence validity (no hallucinations) and Actionability logic.\n",
        "2. Detect Blind Spots (Completeness Check):\n",
        "   - Read the \"original_transcript\" yourself.\n",
        "   - Identify if there are any MAJOR, undeniable business signals (e.g., severe churn risk, distinct product bugs, explicit creator frustration) that the Extractor completely missed.\n",
        "   - Do NOT penalize for missing trivial details, only critical business signals.\n",
        "\n",
        "SCORING RULES:\n",
        "- Evaluate \"extraction_quality_score\" (0.0 to 10.0) based on accuracy of the existing signals.\n",
        "- Evaluate \"completeness_score\" (0.0 to 10.0). Deduct points heavily if major signals were missed.\n",
        "\n",
        "JSON OUTPUT SCHEMA (MANDATORY):\n",
        "{\n",
        "  \"transcript_is_valid\": <boolean>,\n",
        "  \"extraction_quality_score\": <float>,\n",
        "  \"completeness_score\": <float>,\n",
        "  \"missed_signals_detected\": <boolean>,\n",
        "  \"missed_signals_summary\": [\"<string: Description of a major signal that was missed>\", ...],\n",
        "  \"auditor_feedback\": \"<string: Overall feedback on accuracy and completeness>\"\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rZG3lSHy9nL-"
      },
      "id": "rZG3lSHy9nL-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Helper Functions"
      ],
      "metadata": {
        "id": "UjF1In8u9qAM"
      },
      "id": "UjF1In8u9qAM"
    },
    {
      "cell_type": "code",
      "id": "niFtBbXQkiRSCB49okHSalaD",
      "metadata": {
        "tags": [],
        "id": "niFtBbXQkiRSCB49okHSalaD"
      },
      "source": [
        "# ==========================================\n",
        "# 3. HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def parse_raw_transcript(raw_text: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parses the raw text blob from BQ into the list structure required by the Validator.\n",
        "    Handles JSON arrays or simple Line-by-Line text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Attempt 1: Parse as pure JSON\n",
        "        clean_text = re.sub(r\"^```(?:json)?\\s*\", \"\", raw_text)\n",
        "        clean_text = re.sub(r\"\\s*```$\", \"\", clean_text)\n",
        "        return json.loads(clean_text)\n",
        "    except json.JSONDecodeError:\n",
        "        # Attempt 2: Parse Line-by-Line (Fallback)\n",
        "        dialogue = []\n",
        "        lines = raw_text.splitlines()\n",
        "        for line in lines:\n",
        "            if \":\" in line:\n",
        "                parts = line.split(\":\", 1)\n",
        "                role = parts[0].strip()\n",
        "                content = parts[1].strip()\n",
        "                # Normalize roles for the validator\n",
        "                if \"SPM\" in role: role = \"SPM\"\n",
        "                if \"Creator\" in role: role = \"Creator\"\n",
        "                dialogue.append({\"role\": role, \"content\": content})\n",
        "        return dialogue\n",
        "\n",
        "def initialize_destination_table():\n",
        "    \"\"\"\n",
        "    Creates the destination table (Table B) if it doesn't exist yet.\n",
        "    Required so the 'Left Join' in the fetch step doesn't fail.\n",
        "    \"\"\"\n",
        "    table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
        "\n",
        "    # Define the schema for the standalone validation table\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"transcript_id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"transcript_is_valid\", \"BOOLEAN\"),\n",
        "        bigquery.SchemaField(\"extraction_quality_score\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"completeness_score\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"missed_signals_detected\", \"BOOLEAN\"),\n",
        "        bigquery.SchemaField(\"missed_signals_summary\", \"STRING\", mode=\"REPEATED\"), # Array de strings\n",
        "        bigquery.SchemaField(\"validation_report\", \"JSON\"),\n",
        "        bigquery.SchemaField(\"audit_timestamp\", \"TIMESTAMP\"),\n",
        "        bigquery.SchemaField(\"spm_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"creator_id\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"creator_region\", \"STRING\")\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        bq_client.get_table(table_ref)\n",
        "        print(f\"‚úÖ Destination table {DESTINATION_TABLE} exists.\")\n",
        "    except Exception:\n",
        "        print(f\"‚ö†Ô∏è Destination table not found. Creating {DESTINATION_TABLE}...\")\n",
        "        table = bigquery.Table(table_ref, schema=schema)\n",
        "        bq_client.create_table(table)\n",
        "        print(\"‚úÖ Table created successfully.\")\n",
        "\n",
        "def clean_json_response(response_text: str) -> Dict[str, Any]:\n",
        "    try:\n",
        "        cleaned = re.sub(r\"^```(?:json)?\\s*\", \"\", response_text).strip()\n",
        "        cleaned = re.sub(r\"\\s*```$\", \"\", cleaned).strip()\n",
        "        return json.loads(cleaned)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå JSON Parsing Error: {e}\")\n",
        "        # El Fallback debe coincidir con el System Prompt nuevo\n",
        "        return {\n",
        "            \"transcript_is_valid\": False,\n",
        "            \"extraction_quality_score\": 0.0,\n",
        "            \"completeness_score\": 0.0,\n",
        "            \"missed_signals_detected\": False,\n",
        "            \"missed_signals_summary\": [\"Validator Output Malformed\"],\n",
        "            \"auditor_feedback\": \"Error parsing LLM response.\"\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Core Logic: The Validator Class"
      ],
      "metadata": {
        "id": "_drXH0CD9s-b"
      },
      "id": "_drXH0CD9s-b"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. Core Logic: The Validator Class\n",
        "# ==========================================\n",
        "\n",
        "class TranscriptValidator:\n",
        "    def __init__(self):\n",
        "        self.processed_count = 0\n",
        "        self.validation_results = []\n",
        "\n",
        "    def validate_row(self, row) -> Dict[str, Any]:\n",
        "        conv_id = row[\"transcript_id\"]\n",
        "        raw_text = row[\"raw_transcript\"]\n",
        "\n",
        "        # We convert the BQ array to a list of dictionaries in Python\n",
        "        signals_array = [dict(s) for s in row[\"extracted_signals\"]]\n",
        "\n",
        "        payload = {\n",
        "            \"original_transcript\": raw_text,\n",
        "            \"extracted_signals\": signals_array\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            prompt = f\"{VALIDATOR_SYSTEM_PROMPT}\\n\\nUSER INPUT:\\n```json\\n{json.dumps(payload)}\\n```\"\n",
        "\n",
        "            response = model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=GenerationConfig(\n",
        "                    temperature=0.0,\n",
        "                    response_mime_type=\"application/json\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "            audit_report = clean_json_response(response.text)\n",
        "\n",
        "            return {\n",
        "                \"transcript_id\": conv_id,\n",
        "                \"transcript_is_valid\": audit_report.get(\"transcript_is_valid\", False),\n",
        "                \"extraction_quality_score\": audit_report.get(\"extraction_quality_score\", 0.0),\n",
        "                \"completeness_score\": audit_report.get(\"completeness_score\", 0.0),\n",
        "                \"missed_signals_detected\": audit_report.get(\"missed_signals_detected\", False),\n",
        "                \"missed_signals_summary\": audit_report.get(\"missed_signals_summary\", []),\n",
        "                \"validation_report\": json.dumps(audit_report),\n",
        "                \"audit_timestamp\": datetime.utcnow().isoformat()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error validating {conv_id}: {str(e)}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "UQjfNoRr9tOj"
      },
      "id": "UQjfNoRr9tOj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. BigQuery Upload"
      ],
      "metadata": {
        "id": "NzyRo15Q9w1Z"
      },
      "id": "NzyRo15Q9w1Z"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. BIGQUERY UPLOAD (DELTA LOGIC)\n",
        "# ==========================================\n",
        "\n",
        "def fetch_pending_transcripts(limit: int = 100):\n",
        "    \"\"\"\n",
        "    Selects rows from Source (A) that are NOT present in Destination (B).\n",
        "    Pattern: LEFT JOIN ... WHERE B.id IS NULL\n",
        "    \"\"\"\n",
        "    query = f\"\"\"\n",
        "        SELECT\n",
        "            orig.conversation_id AS transcript_id,\n",
        "            orig.raw_transcript,\n",
        "            orig.spm_name,\n",
        "            orig.creator_region,\n",
        "            ARRAY_AGG(\n",
        "                STRUCT(\n",
        "                    ext.signal_name,\n",
        "                    ext.signal_category,\n",
        "                    ext.signal_actionability,\n",
        "                    ext.signal_description,\n",
        "                    ext.signal_evidence,\n",
        "                    ext.spm_score,\n",
        "                    ext.spm_reasoning\n",
        "                )\n",
        "            ) as extracted_signals\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}` ext\n",
        "        JOIN `{PROJECT_ID}.{DATASET_ID}.{TRANSCRIPTS_TABLE}` orig\n",
        "          ON ext.transcript_id = orig.conversation_id\n",
        "        LEFT JOIN `{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}` val\n",
        "          ON ext.transcript_id = val.transcript_id\n",
        "        WHERE val.transcript_id IS NULL\n",
        "        GROUP BY orig.conversation_id, orig.raw_transcript, orig.spm_name, orig.creator_region\n",
        "        LIMIT {limit}\n",
        "    \"\"\"\n",
        "    print(\"üì• Fetching pending grouped transcripts (Delta Load)...\")\n",
        "    return bq_client.query(query).result()\n",
        "\n",
        "def upload_results_to_bq(results: List[Dict[str, Any]]):\n",
        "    \"\"\"\n",
        "    Inserts validated rows directly into Destination Table.\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        return\n",
        "\n",
        "    table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
        "\n",
        "    # Schema must match initialize_destination_table\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        write_disposition=\"WRITE_APPEND\",\n",
        "        schema=[\n",
        "            bigquery.SchemaField(\"transcript_id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "            bigquery.SchemaField(\"transcript_is_valid\", \"BOOLEAN\"),\n",
        "            bigquery.SchemaField(\"extraction_quality_score\", \"FLOAT\"),\n",
        "            bigquery.SchemaField(\"completeness_score\", \"FLOAT\"),\n",
        "            bigquery.SchemaField(\"missed_signals_detected\", \"BOOLEAN\"),\n",
        "            bigquery.SchemaField(\"missed_signals_summary\", \"STRING\", mode=\"REPEATED\"), # Array de strings\n",
        "            bigquery.SchemaField(\"validation_report\", \"JSON\"),\n",
        "            bigquery.SchemaField(\"audit_timestamp\", \"TIMESTAMP\"),\n",
        "            bigquery.SchemaField(\"spm_name\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"creator_id\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"creator_region\", \"STRING\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        job = bq_client.load_table_from_json(results, table_ref, job_config=job_config)\n",
        "        job.result() # Wait for completion\n",
        "        print(f\"‚úÖ Saved {len(results)} audited records to {DESTINATION_TABLE}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Upload failed: {e}\")"
      ],
      "metadata": {
        "id": "gb8_PkXH9xBo"
      },
      "id": "gb8_PkXH9xBo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Main"
      ],
      "metadata": {
        "id": "e4HKuRwCNt3c"
      },
      "id": "e4HKuRwCNt3c"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import sys # Import needed for clean exit\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    print(\"üöÄ Starting Validator Pipeline (Delta Strategy)...\")\n",
        "\n",
        "    # 1. Initialize Destination Table (Crucial Step)\n",
        "    initialize_destination_table()\n",
        "\n",
        "    validator = TranscriptValidator()\n",
        "\n",
        "    # 2. Fetch Data (The Delta)\n",
        "    rows = fetch_pending_transcripts(limit=120)\n",
        "    rows_list = list(rows)\n",
        "\n",
        "    if not rows_list:\n",
        "        print(\"üò¥ No pending signals found (All rows in Source are already in Destination).\")\n",
        "        sys.exit()\n",
        "\n",
        "    print(f\"üîç Found {len(rows_list)} new signals to validate.\")\n",
        "\n",
        "    batch_results = []\n",
        "\n",
        "    # 3. Iterate and Validate\n",
        "    for row in rows_list:\n",
        "        result = validator.validate_row(row)\n",
        "\n",
        "        if result:\n",
        "            result[\"spm_name\"] = row.get(\"spm_name\")\n",
        "            result[\"creator_id\"] = row.get(\"creator_id\")\n",
        "            result[\"creator_region\"] = row.get(\"creator_region\")\n",
        "\n",
        "            batch_results.append(result)\n",
        "\n",
        "            # logging\n",
        "            status_icon = \"‚úÖ\" if result[\"transcript_is_valid\"] else \"‚ùå\"\n",
        "            print(f\"{status_icon} [{result['transcript_id']}] Q-Score: {result['extraction_quality_score']} | Completeness: {result['completeness_score']}\")\n",
        "\n",
        "        # Rate Limiting\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    end_time_sd = time.perf_counter()\n",
        "\n",
        "    # 4. Upload Results\n",
        "    if batch_results:\n",
        "        upload_results_to_bq(batch_results)\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "\n",
        "    print(\"üèÅ Pipeline finished successfully.\")\n",
        "\n",
        "    duration_sd = str(timedelta(seconds=end_time_sd - start_time))\n",
        "    duration_bq = str(timedelta(seconds=end_time - end_time_sd))\n",
        "    duration_total = str(timedelta(seconds=end_time - start_time))\n",
        "\n",
        "    print(f\"Validation execution time: {duration_sd}\")\n",
        "    print(f\"Upload execution time: {duration_bq}\")\n",
        "    print(f\"Total execution time: {duration_total}\")"
      ],
      "metadata": {
        "id": "RZtiauz0Nwwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d191164-2989-4479-c74c-e51451f2cb1f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1771564344157,
          "user_tz": 360,
          "elapsed": 8,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "RZtiauz0Nwwg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Validator Pipeline (Delta Strategy)...\n",
            "‚ö†Ô∏è Destination table not found. Creating vel_csv_signals_validation_001...\n",
            "‚úÖ Table created successfully.\n",
            "üì• Fetching pending grouped transcripts (Delta Load)...\n",
            "üîç Found 72 new signals to validate.\n",
            "‚úÖ [1_0_1771463705] Q-Score: 9.5 | Completeness: 9.8\n",
            "‚úÖ [28_0_1771465305] Q-Score: 10.0 | Completeness: 6.5\n",
            "‚úÖ [41_0_1771466063] Q-Score: 10.0 | Completeness: 8.0\n",
            "‚úÖ [72_0_1771467872] Q-Score: 5.0 | Completeness: 3.0\n",
            "‚úÖ [72_1_1771467932] Q-Score: 9.5 | Completeness: 7.0\n",
            "‚úÖ [8_0_1771464161] Q-Score: 9.5 | Completeness: 7.0\n",
            "‚úÖ [8_1_1771464210] Q-Score: 10.0 | Completeness: 10.0\n",
            "‚úÖ [30_0_1771465427] Q-Score: 10.0 | Completeness: 10.0\n",
            "‚úÖ [39_0_1771465931] Q-Score: 9.5 | Completeness: 9.5\n",
            "‚úÖ [3_0_1771463776] Q-Score: 9.5 | Completeness: 7.5\n",
            "‚úÖ [3_1_1771463831] Q-Score: 10.0 | Completeness: 5.0\n",
            "‚úÖ [14_0_1771464510] Q-Score: 5.0 | Completeness: 6.0\n",
            "‚úÖ [31_0_1771465461] Q-Score: 9.8 | Completeness: 6.5\n",
            "‚úÖ [62_0_1771467201] Q-Score: 9.5 | Completeness: 10.0\n",
            "‚úÖ [73_0_1771467969] Q-Score: 9.0 | Completeness: 4.0\n",
            "‚úÖ [11_0_1771464368] Q-Score: 6.5 | Completeness: 8.0\n",
            "‚úÖ [60_0_1771467085] Q-Score: 10.0 | Completeness: 9.5\n",
            "‚úÖ [60_1_1771467116] Q-Score: 10.0 | Completeness: 7.0\n",
            "‚úÖ [67_0_1771467463] Q-Score: 10.0 | Completeness: 6.0\n",
            "‚úÖ [13_0_1771464461] Q-Score: 9.5 | Completeness: 9.5\n",
            "‚úÖ [38_0_1771465867] Q-Score: 9.8 | Completeness: 9.5\n",
            "‚úÖ [38_1_1771465902] Q-Score: 9.5 | Completeness: 6.5\n",
            "‚úÖ [18_0_1771464705] Q-Score: 9.8 | Completeness: 9.0\n",
            "‚úÖ [18_1_1771464746] Q-Score: 9.5 | Completeness: 10.0\n",
            "‚úÖ [68_1_1771467565] Q-Score: 9.5 | Completeness: 4.0\n",
            "‚úÖ [21_0_1771464914] Q-Score: 8.0 | Completeness: 9.5\n",
            "‚úÖ [21_1_1771464950] Q-Score: 9.5 | Completeness: 5.5\n",
            "‚úÖ [29_0_1771465357] Q-Score: 10.0 | Completeness: 10.0\n",
            "‚úÖ [29_1_1771465393] Q-Score: 9.5 | Completeness: 10.0\n",
            "‚úÖ [23_0_1771465054] Q-Score: 10.0 | Completeness: 10.0\n",
            "‚úÖ [33_0_1771465549] Q-Score: 9.5 | Completeness: 7.5\n",
            "‚úÖ [65_0_1771467375] Q-Score: 9.0 | Completeness: 10.0\n",
            "‚úÖ [16_0_1771464574] Q-Score: 9.5 | Completeness: 9.5\n",
            "‚úÖ [16_1_1771464623] Q-Score: 9.5 | Completeness: 7.0\n",
            "‚úÖ [17_0_1771464669] Q-Score: 9.5 | Completeness: 10.0\n",
            "‚úÖ [27_1_1771465267] Q-Score: 10.0 | Completeness: 9.0\n",
            "‚úÖ [35_0_1771465641] Q-Score: 10.0 | Completeness: 7.5\n",
            "‚úÖ [35_1_1771465687] Q-Score: 10.0 | Completeness: 10.0\n",
            "‚úÖ [40_0_1771466017] Q-Score: 9.5 | Completeness: 9.5\n",
            "‚úÖ [45_0_1771466262] Q-Score: 9.5 | Completeness: 7.5\n",
            "‚úÖ [54_0_1771466758] Q-Score: 9.5 | Completeness: 7.5\n",
            "‚úÖ [58_0_1771466993] Q-Score: 10.0 | Completeness: 5.0\n",
            "‚úÖ [4_0_1771463882] Q-Score: 6.0 | Completeness: 6.0\n",
            "‚úÖ [10_0_1771464321] Q-Score: 7.0 | Completeness: 10.0\n",
            "‚úÖ [56_0_1771466913] Q-Score: 10.0 | Completeness: 7.5\n",
            "‚úÖ [15_0_1771464542] Q-Score: 10.0 | Completeness: 9.5\n",
            "‚úÖ [20_0_1771464833] Q-Score: 10.0 | Completeness: 10.0\n",
            "‚úÖ [20_1_1771464875] Q-Score: 9.5 | Completeness: 7.0\n",
            "‚úÖ [51_0_1771466595] Q-Score: 10.0 | Completeness: 6.0\n",
            "‚úÖ [55_0_1771466806] Q-Score: 10.0 | Completeness: 10.0\n",
            "‚úÖ [55_1_1771466860] Q-Score: 10.0 | Completeness: 10.0\n",
            "‚úÖ [2_0_1771463740] Q-Score: 9.5 | Completeness: 8.0\n",
            "‚úÖ [42_0_1771466105] Q-Score: 10.0 | Completeness: 10.0\n",
            "‚úÖ [46_0_1771466304] Q-Score: 10.0 | Completeness: 9.0\n",
            "‚úÖ [70_1_1771467719] Q-Score: 5.0 | Completeness: 8.5\n",
            "‚úÖ [71_0_1771467758] Q-Score: 9.5 | Completeness: 10.0\n",
            "‚úÖ [71_1_1771467818] Q-Score: 9.8 | Completeness: 9.5\n",
            "‚úÖ [7_1_1771464113] Q-Score: 9.0 | Completeness: 5.0\n",
            "‚úÖ [36_0_1771465719] Q-Score: 8.0 | Completeness: 10.0\n",
            "‚úÖ [50_0_1771466500] Q-Score: 9.8 | Completeness: 9.8\n",
            "‚úÖ [50_1_1771466541] Q-Score: 9.5 | Completeness: 9.5\n",
            "‚úÖ [9_0_1771464246] Q-Score: 10.0 | Completeness: 9.5\n",
            "‚úÖ [7_1_1771531308] Q-Score: 5.0 | Completeness: 9.0\n",
            "‚úÖ [6_0_1771531227] Q-Score: 8.0 | Completeness: 6.0\n",
            "‚úÖ [9_1_1771531512] Q-Score: 9.5 | Completeness: 10.0\n",
            "‚úÖ [10_0_1771531556] Q-Score: 8.0 | Completeness: 5.0\n",
            "‚úÖ [15_0_1771531852] Q-Score: 9.0 | Completeness: 6.5\n",
            "‚úÖ [1_0_1771530949] Q-Score: 10.0 | Completeness: 10.0\n",
            "‚úÖ [1_1_1771530986] Q-Score: 6.0 | Completeness: 5.5\n",
            "‚úÖ [2_0_1771531023] Q-Score: 9.0 | Completeness: 9.5\n",
            "‚úÖ [3_0_1771531075] Q-Score: 8.5 | Completeness: 2.0\n",
            "‚úÖ [4_0_1771531118] Q-Score: 7.5 | Completeness: 5.5\n",
            "‚úÖ Saved 72 audited records to vel_csv_signals_validation_001\n",
            "üèÅ Pipeline finished successfully.\n",
            "Validation execution time: 0:19:07.656630\n",
            "Upload execution time: 0:00:02.488548\n",
            "Total execution time: 0:19:10.145177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In average it takes ~17 seconds to validate all extracted signals and missing signals from 1 transcription"
      ],
      "metadata": {
        "id": "SxsrJFykR9X4"
      },
      "id": "SxsrJFykR9X4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "vel_csv_signals_validator_001"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}