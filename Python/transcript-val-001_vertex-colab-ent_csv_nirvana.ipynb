{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Main Code"
      ],
      "metadata": {
        "id": "bjU7yc3ANkH_"
      },
      "id": "bjU7yc3ANkH_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Libraries"
      ],
      "metadata": {
        "id": "-nOP2Av89eWT"
      },
      "id": "-nOP2Av89eWT"
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import GoogleAPIError\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Any, Optional"
      ],
      "metadata": {
        "id": "edbUQVlo9cqc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1771560782024,
          "user_tz": 360,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "edbUQVlo9cqc",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Configuration & Naming Standards"
      ],
      "metadata": {
        "id": "KKhEzA4C9hzJ"
      },
      "id": "KKhEzA4C9hzJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. CONFIGURATION & NAMING STANDARDS\n",
        "# ==========================================\n",
        "\n",
        "PROJECT_ID = \"project-nirvana-405904\"  # <--- REPLACE THIS\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "PROJECT_TAG = \"csv\"\n",
        "SCRIPT_VERSION = \"001\"\n",
        "SOURCE_VERSION = \"005\" # <--- CHANGE THIS TO THE CURRENT TRANSCRIPTIONS TABLE VERSION\n",
        "DESTINATION_TABLE_VERSION = \"001\"\n",
        "\n",
        "JSONL_FILE = f\"vel_{PROJECT_TAG}_validations_{SCRIPT_VERSION}.jsonl\"\n",
        "DATASET_ID = f\"vel_{PROJECT_TAG}_schema\"\n",
        "SOURCE_TABLE = f\"vel_{PROJECT_TAG}_synthetic_transcripts_{SOURCE_VERSION}\"\n",
        "DESTINATION_TABLE = f\"vel_{PROJECT_TAG}_transcripts_validation_{DESTINATION_TABLE_VERSION}\"\n",
        "\n",
        "# Initialize Vertex AI\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "model = GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "# Initialize BigQuery\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)"
      ],
      "metadata": {
        "id": "3m1YdXlR9iNx",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1771560782024,
          "user_tz": 360,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "3m1YdXlR9iNx",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Validator System Promopt (Strict Auditor)"
      ],
      "metadata": {
        "id": "JZkNBYaT9m8n"
      },
      "id": "JZkNBYaT9m8n"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. VALIDATOR SYSTEM PROMPT (Strict Auditor)\n",
        "# ==========================================\n",
        "\n",
        "VALIDATOR_SYSTEM_PROMPT = \"\"\"\n",
        "You are an automated **Agent Validator**: a strict, forensic auditor for synthetic conversational transcripts between:\n",
        " - Agent A: \"YouTube Strategic Partner Manager\" (SPM)\n",
        " - Agent B: \"YouTube Creator\"\n",
        "\n",
        "Your job: read exactly one JSON payload (transcript + metadata) provided as the user message content, evaluate it against the rules below, and output **only** a single JSON object that strictly follows the JSON Output Schema.\n",
        "\n",
        "REQUIREMENTS\n",
        "1. Parse input JSON; expect keys: \"transcript\", \"meta\", \"conversation_id\".\n",
        "2. Treat yourself as a strict auditor. If input is malformed, return is_valid=false.\n",
        "\n",
        "3. Run the following detectors:\n",
        "   A. Hallucination Detector (PRODUCT & POLICY PROMISES):\n",
        "      - Identify claims by SPM promising impossible outcomes (e.g., \"I will manually remove a strike in 5 minutes\").\n",
        "      - Flag \"absolute timelines\" (verbs + specific times) for administrative actions as suspicious.\n",
        "   B. Character Coherence Detector:\n",
        "      - Compare utterance to persona. Flag mismatch (e.g., SPM using insults).\n",
        "   C. Policy Accuracy Detector:\n",
        "      - Check for obviously incorrect instructions (e.g., \"delete account without password\").\n",
        "   D. Conversational Realism Detector (Human-ness):\n",
        "      - Score naturalness (1.0-5.0).\n",
        "      - PENALIZE \"Robot-talk\": Perfect grammar, zero fillers, no interruptions.\n",
        "      - REWARD \"Human noise\": Fillers (\"uh\", \"basically\"), interruptions, hedging.\n",
        "   E. Structural Narrative Detector:\n",
        "      - Ensure start -> problem -> resolution flow.\n",
        "\n",
        "SCORING RULES:\n",
        "- Realism_Human, Policy_Precision, Narrative_Coherence (1.0-5.0).\n",
        "- quality_score: weighted average = round( Realism_Human*0.35 + Policy_Precision*0.40 + Narrative_Coherence*0.25 , 2 )\n",
        "- If hallucination_flag == true, reduce quality_score by 1.0.\n",
        "- is_valid == true ONLY if: quality_score >= 3.2 AND hallucination_flag == false.\n",
        "\n",
        "JSON OUTPUT SCHEMA (MANDATORY):\n",
        "{\n",
        "  \"is_valid\": <boolean>,\n",
        "  \"quality_score\": <float>,\n",
        "  \"hallucination_flag\": <boolean>,\n",
        "  \"detected_errors\": [<string>, ...],\n",
        "  \"correction_suggestions\": [<string>, ...],\n",
        "  \"Realism_Human\": <float or null>,\n",
        "  \"Policy_Precision\": <float or null>,\n",
        "  \"Narrative_Coherence\": <float or null>\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rZG3lSHy9nL-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1771560782024,
          "user_tz": 360,
          "elapsed": 4,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "rZG3lSHy9nL-",
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Helper Functions"
      ],
      "metadata": {
        "id": "UjF1In8u9qAM"
      },
      "id": "UjF1In8u9qAM"
    },
    {
      "cell_type": "code",
      "id": "niFtBbXQkiRSCB49okHSalaD",
      "metadata": {
        "tags": [],
        "id": "niFtBbXQkiRSCB49okHSalaD",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1771560782024,
          "user_tz": 360,
          "elapsed": 4,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "source": [
        "# ==========================================\n",
        "# 3. HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def parse_raw_transcript(raw_text: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parses the raw text blob from BQ into the list structure required by the Validator.\n",
        "    Handles JSON arrays or simple Line-by-Line text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Attempt 1: Parse as pure JSON\n",
        "        clean_text = re.sub(r\"^```(?:json)?\\s*\", \"\", raw_text)\n",
        "        clean_text = re.sub(r\"\\s*```$\", \"\", clean_text)\n",
        "        return json.loads(clean_text)\n",
        "    except json.JSONDecodeError:\n",
        "        # Attempt 2: Parse Line-by-Line (Fallback)\n",
        "        dialogue = []\n",
        "        lines = raw_text.splitlines()\n",
        "        for line in lines:\n",
        "            if \":\" in line:\n",
        "                parts = line.split(\":\", 1)\n",
        "                role = parts[0].strip()\n",
        "                content = parts[1].strip()\n",
        "                # Normalize roles for the validator\n",
        "                if \"SPM\" in role: role = \"SPM\"\n",
        "                if \"Creator\" in role: role = \"Creator\"\n",
        "                dialogue.append({\"role\": role, \"content\": content})\n",
        "        return dialogue\n",
        "\n",
        "def initialize_destination_table():\n",
        "    \"\"\"\n",
        "    Creates the destination table (Table B) if it doesn't exist yet.\n",
        "    Required so the 'Left Join' in the fetch step doesn't fail.\n",
        "    \"\"\"\n",
        "    table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
        "\n",
        "    # Define the schema for the standalone validation table\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"conversation_id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "        bigquery.SchemaField(\"is_valid\", \"BOOLEAN\"),\n",
        "        bigquery.SchemaField(\"quality_score\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"hallucination_flag\", \"BOOLEAN\"),\n",
        "        bigquery.SchemaField(\"validation_report\", \"JSON\"),\n",
        "        bigquery.SchemaField(\"audit_timestamp\", \"TIMESTAMP\"),\n",
        "        # Metadata columns (copied from source for easier analysis)\n",
        "        bigquery.SchemaField(\"spm_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"creator_id\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"creator_region\", \"STRING\")\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        bq_client.get_table(table_ref)\n",
        "        print(f\"‚úÖ Destination table {DESTINATION_TABLE} exists.\")\n",
        "    except Exception:\n",
        "        print(f\"‚ö†Ô∏è Destination table not found. Creating {DESTINATION_TABLE}...\")\n",
        "        table = bigquery.Table(table_ref, schema=schema)\n",
        "        bq_client.create_table(table)\n",
        "        print(\"‚úÖ Table created successfully.\")\n",
        "\n",
        "def clean_json_response(response_text: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Sanitizes LLM output to ensure valid JSON parsing.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        cleaned = re.sub(r\"^```(?:json)?\\s*\", \"\", response_text).strip()\n",
        "        cleaned = re.sub(r\"\\s*```$\", \"\", cleaned).strip()\n",
        "        return json.loads(cleaned)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå JSON Parsing Error: {e}\")\n",
        "        return {\n",
        "            \"is_valid\": False,\n",
        "            \"quality_score\": 1.0,\n",
        "            \"hallucination_flag\": True,\n",
        "            \"detected_errors\": [\"Validator Output Malformed\"],\n",
        "            \"Realism_Human\": 0.0,\n",
        "            \"Policy_Precision\": 0.0,\n",
        "            \"Narrative_Coherence\": 0.0\n",
        "        }"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Core Logic: The Validator Class"
      ],
      "metadata": {
        "id": "_drXH0CD9s-b"
      },
      "id": "_drXH0CD9s-b"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. Core Logic: The Validator Class\n",
        "# ==========================================\n",
        "\n",
        "class TranscriptValidator:\n",
        "    def __init__(self):\n",
        "        self.processed_count = 0\n",
        "        self.validation_results = []\n",
        "\n",
        "    def validate_row(self, row) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Takes a BigQuery Row object, runs validation, returns the result dict.\n",
        "        \"\"\"\n",
        "        conv_id = row[\"conversation_id\"]\n",
        "        raw_text = row[\"raw_transcript\"]\n",
        "\n",
        "        # 1. Parse Input\n",
        "        transcript_data = parse_raw_transcript(raw_text)\n",
        "\n",
        "        # 2. Construct Payload\n",
        "        # Note: We assume 'meta' exists in BQ row, or we construct a basic one\n",
        "        payload = {\n",
        "            \"conversation_id\": conv_id,\n",
        "            \"transcript\": transcript_data,\n",
        "            \"meta\": {\n",
        "                \"role_label\": {\"A\": \"SPM\", \"B\": \"Creator\"},\n",
        "                # If these fields are missing in BQ, we provide defaults to avoid Validator crash\n",
        "                \"channel\": row.get(\"creator_id\", \"Unknown\"),\n",
        "                \"region\": row.get(\"creator_region\", \"Unknown\")\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # 3. Call Gemini\n",
        "        try:\n",
        "            prompt = f\"{VALIDATOR_SYSTEM_PROMPT}\\n\\nUSER INPUT:\\n```json\\n{json.dumps(payload)}\\n```\"\n",
        "\n",
        "            response = model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=GenerationConfig(\n",
        "                    temperature=0.0, # Deterministic for auditing\n",
        "                    response_mime_type=\"application/json\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # 4. Process Output\n",
        "            audit_report = clean_json_response(response.text)\n",
        "\n",
        "            # 5. Return Enriched Record\n",
        "            return {\n",
        "                \"conversation_id\": conv_id,\n",
        "                \"is_valid\": audit_report.get(\"is_valid\", False),\n",
        "                \"quality_score\": audit_report.get(\"quality_score\", 0.0),\n",
        "                \"hallucination_flag\": audit_report.get(\"hallucination_flag\", False),\n",
        "                \"validation_report\": json.dumps(audit_report), # Store full JSON as string\n",
        "                \"audit_timestamp\": datetime.utcnow().isoformat()\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error validating {conv_id}: {str(e)}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "UQjfNoRr9tOj",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1771560782024,
          "user_tz": 360,
          "elapsed": 4,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "UQjfNoRr9tOj",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. BigQuery Upload"
      ],
      "metadata": {
        "id": "NzyRo15Q9w1Z"
      },
      "id": "NzyRo15Q9w1Z"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. BIGQUERY UPLOAD (DELTA LOGIC)\n",
        "# ==========================================\n",
        "\n",
        "def fetch_pending_transcripts(limit: int = 100):\n",
        "    \"\"\"\n",
        "    Selects rows from Source (A) that are NOT present in Destination (B).\n",
        "    Pattern: LEFT JOIN ... WHERE B.id IS NULL\n",
        "    \"\"\"\n",
        "    query = f\"\"\"\n",
        "        SELECT\n",
        "            t.conversation_id,\n",
        "            t.raw_transcript,\n",
        "            t.creator_id,\n",
        "            t.creator_region,\n",
        "            t.spm_name\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.{SOURCE_TABLE}` t\n",
        "        LEFT JOIN `{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}` v\n",
        "        ON t.conversation_id = v.conversation_id\n",
        "        WHERE v.conversation_id IS NULL\n",
        "        LIMIT {limit}\n",
        "    \"\"\"\n",
        "    print(\"üì• Fetching pending transcripts (Delta Load)...\")\n",
        "    return bq_client.query(query).result()\n",
        "\n",
        "def upload_results_to_bq(results: List[Dict[str, Any]]):\n",
        "    \"\"\"\n",
        "    Inserts validated rows directly into Destination Table.\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        return\n",
        "\n",
        "    table_ref = f\"{PROJECT_ID}.{DATASET_ID}.{DESTINATION_TABLE}\"\n",
        "\n",
        "    # Schema must match initialize_destination_table\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        write_disposition=\"WRITE_APPEND\",\n",
        "        schema=[\n",
        "            bigquery.SchemaField(\"conversation_id\", \"STRING\", mode=\"REQUIRED\"),\n",
        "            bigquery.SchemaField(\"is_valid\", \"BOOLEAN\"),\n",
        "            bigquery.SchemaField(\"quality_score\", \"FLOAT\"),\n",
        "            bigquery.SchemaField(\"hallucination_flag\", \"BOOLEAN\"),\n",
        "            bigquery.SchemaField(\"validation_report\", \"JSON\"),\n",
        "            bigquery.SchemaField(\"audit_timestamp\", \"TIMESTAMP\"),\n",
        "            bigquery.SchemaField(\"spm_name\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"creator_id\", \"STRING\"),\n",
        "            bigquery.SchemaField(\"creator_region\", \"STRING\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        job = bq_client.load_table_from_json(results, table_ref, job_config=job_config)\n",
        "        job.result() # Wait for completion\n",
        "        print(f\"‚úÖ Saved {len(results)} audited records to {DESTINATION_TABLE}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Upload failed: {e}\")"
      ],
      "metadata": {
        "id": "gb8_PkXH9xBo",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1771560782024,
          "user_tz": 360,
          "elapsed": 4,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "gb8_PkXH9xBo",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Main"
      ],
      "metadata": {
        "id": "e4HKuRwCNt3c"
      },
      "id": "e4HKuRwCNt3c"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import sys # Import needed for clean exit\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    print(\"üöÄ Starting Validator Pipeline (Delta Strategy)...\")\n",
        "\n",
        "    # 1. Initialize Destination Table (Crucial Step)\n",
        "    initialize_destination_table()\n",
        "\n",
        "    validator = TranscriptValidator()\n",
        "\n",
        "    # 2. Fetch Data (The Delta)\n",
        "    rows = fetch_pending_transcripts(limit=120)\n",
        "    rows_list = list(rows)\n",
        "\n",
        "    if not rows_list:\n",
        "        print(\"üò¥ No pending transcripts found (All rows in Source are already in Destination).\")\n",
        "        sys.exit()\n",
        "\n",
        "    print(f\"üîç Found {len(rows_list)} new transcripts to validate.\")\n",
        "\n",
        "    batch_results = []\n",
        "\n",
        "    # 3. Iterate and Validate\n",
        "    for row in rows_list:\n",
        "        result = validator.validate_row(row)\n",
        "\n",
        "        if result:\n",
        "            # ENRICHMENT: Add metadata from Source Row to Result Dict\n",
        "            # This makes the destination table standalone/analytical\n",
        "            result[\"spm_name\"] = row.get(\"spm_name\")\n",
        "            result[\"creator_id\"] = row.get(\"creator_id\")\n",
        "            result[\"creator_region\"] = row.get(\"creator_region\")\n",
        "\n",
        "            batch_results.append(result)\n",
        "\n",
        "            # Logging\n",
        "            status_icon = \"‚úÖ\" if result[\"is_valid\"] else \"‚ùå\"\n",
        "            print(f\"{status_icon} [{row['conversation_id']}] Score: {result['quality_score']}\")\n",
        "\n",
        "        # Rate Limiting\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    end_time_sd = time.perf_counter()\n",
        "\n",
        "    # 4. Upload Results\n",
        "    if batch_results:\n",
        "        upload_results_to_bq(batch_results)\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "\n",
        "    print(\"üèÅ Pipeline finished successfully.\")\n",
        "\n",
        "    duration_sd = str(timedelta(seconds=end_time_sd - start_time))\n",
        "    duration_bq = str(timedelta(seconds=end_time - end_time_sd))\n",
        "    duration_total = str(timedelta(seconds=end_time - start_time))\n",
        "\n",
        "    print(f\"Validation execution time: {duration_sd}\")\n",
        "    print(f\"Upload execution time: {duration_bq}\")\n",
        "    print(f\"Total execution time: {duration_total}\")"
      ],
      "metadata": {
        "id": "RZtiauz0Nwwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "9fccdeb3-e412-45a1-efac-941f8abfdbfa",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1771564344201,
          "user_tz": 360,
          "elapsed": 12,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "RZtiauz0Nwwg",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Validator Pipeline (Delta Strategy)...\n",
            "‚ö†Ô∏è Destination table not found. Creating vel_csv_transcripts_validation_001...\n",
            "‚úÖ Table created successfully.\n",
            "üì• Fetching pending transcripts (Delta Load)...\n",
            "üîç Found 120 new transcripts to validate.\n",
            "‚ùå [11_0_1771531592] Score: 0.0\n",
            "‚ùå [11_1_1771531648] Score: None\n",
            "‚ùå [7_0_1771531273] Score: 3.61\n",
            "‚úÖ [7_1_1771531308] Score: 4.7\n",
            "‚ùå [12_0_1771531701] Score: None\n",
            "‚ùå [5_0_1771531169] Score: 0.0\n",
            "‚úÖ [6_0_1771531227] Score: 4.93\n",
            "‚ùå [9_0_1771531453] Score: None\n",
            "‚úÖ [9_1_1771531512] Score: 4.88\n",
            "‚úÖ [10_0_1771531556] Score: 4.93\n",
            "‚úÖ [15_0_1771531852] Score: 4.93\n",
            "‚úÖ [1_0_1771530949] Score: 4.93\n",
            "‚úÖ [1_1_1771530986] Score: 4.86\n",
            "‚ùå [13_0_1771531757] Score: None\n",
            "‚úÖ [2_0_1771531023] Score: 4.35\n",
            "‚úÖ [3_0_1771531075] Score: 4.83\n",
            "‚úÖ [4_0_1771531118] Score: 4.83\n",
            "‚ùå [8_0_1771531350] Score: 0.0\n",
            "‚ùå [8_1_1771531407] Score: 3.83\n",
            "‚ùå [14_0_1771531814] Score: 3.68\n",
            "‚úÖ [1_0_1771463705] Score: 4.9\n",
            "‚úÖ [28_0_1771465305] Score: 4.97\n",
            "‚úÖ [41_0_1771466063] Score: 4.93\n",
            "‚ùå [49_0_1771466441] Score: 0.0\n",
            "‚úÖ [72_0_1771467872] Score: 4.93\n",
            "‚úÖ [72_1_1771467932] Score: 4.93\n",
            "‚úÖ [8_0_1771464161] Score: 4.97\n",
            "‚úÖ [8_1_1771464210] Score: 4.7\n",
            "‚ùå [61_0_1771467150] Score: None\n",
            "‚úÖ [30_0_1771465427] Score: 4.88\n",
            "‚úÖ [39_0_1771465931] Score: 4.93\n",
            "‚ùå [39_1_1771465964] Score: 3.93\n",
            "‚ùå [59_0_1771467031] Score: 3.93\n",
            "‚ùå [64_0_1771467280] Score: None\n",
            "‚ùå [64_1_1771467339] Score: 0.0\n",
            "‚úÖ [3_0_1771463776] Score: 4.78\n",
            "‚úÖ [3_1_1771463831] Score: 4.93\n",
            "‚ùå [44_0_1771466205] Score: None\n",
            "‚ùå [66_0_1771467409] Score: 0.0\n",
            "‚ùå [12_0_1771464410] Score: None\n",
            "‚úÖ [14_0_1771464510] Score: 5.0\n",
            "‚ùå [26_0_1771465190] Score: 3.67\n",
            "‚úÖ [31_0_1771465461] Score: 4.78\n",
            "‚úÖ [62_0_1771467201] Score: 4.6\n",
            "‚úÖ [73_0_1771467969] Score: 4.83\n",
            "‚úÖ [11_0_1771464368] Score: 4.6\n",
            "‚úÖ [60_0_1771467085] Score: 4.83\n",
            "‚úÖ [60_1_1771467116] Score: 4.77\n",
            "‚úÖ [67_0_1771467463] Score: 4.67\n",
            "‚úÖ [13_0_1771464461] Score: 4.83\n",
            "‚ùå [32_0_1771465502] Score: None\n",
            "‚úÖ [38_0_1771465867] Score: 4.78\n",
            "‚úÖ [38_1_1771465902] Score: 4.91\n",
            "‚ùå [53_0_1771466718] Score: 3.8\n",
            "‚úÖ [18_0_1771464705] Score: 4.84\n",
            "‚úÖ [18_1_1771464746] Score: 4.93\n",
            "‚ùå [68_0_1771467523] Score: None\n",
            "‚úÖ [68_1_1771467565] Score: 4.86\n",
            "‚úÖ [21_0_1771464914] Score: 4.85\n",
            "‚úÖ [21_1_1771464950] Score: 4.93\n",
            "‚úÖ [29_0_1771465357] Score: 4.93\n",
            "‚úÖ [29_1_1771465393] Score: 4.83\n",
            "‚ùå [48_0_1771466389] Score: None\n",
            "‚ùå [6_0_1771464014] Score: None\n",
            "‚úÖ [23_0_1771465054] Score: 4.7\n",
            "‚úÖ [33_0_1771465549] Score: 4.68\n",
            "‚úÖ [65_0_1771467375] Score: 4.62\n",
            "‚ùå [5_0_1771463978] Score: 3.7\n",
            "‚úÖ [16_0_1771464574] Score: 4.86\n",
            "‚úÖ [16_1_1771464623] Score: 4.82\n",
            "‚ùå [75_0_1771468123] Score: 3.86\n",
            "‚úÖ [17_0_1771464669] Score: 4.7\n",
            "‚ùå [27_0_1771465220] Score: 3.81\n",
            "‚úÖ [27_1_1771465267] Score: 4.81\n",
            "‚ùå [34_0_1771465583] Score: 0.0\n",
            "‚úÖ [35_0_1771465641] Score: 4.86\n",
            "‚úÖ [35_1_1771465687] Score: 4.93\n",
            "‚úÖ [40_0_1771466017] Score: 4.86\n",
            "‚úÖ [45_0_1771466262] Score: 4.6\n",
            "‚úÖ [54_0_1771466758] Score: 4.91\n",
            "‚úÖ [58_0_1771466993] Score: 4.8\n",
            "‚ùå [63_0_1771467235] Score: 3.7\n",
            "‚úÖ [4_0_1771463882] Score: 4.94\n",
            "‚ùå [4_1_1771463931] Score: 3.88\n",
            "‚úÖ [10_0_1771464321] Score: 4.7\n",
            "‚ùå [22_0_1771465002] Score: 0.0\n",
            "‚úÖ [56_0_1771466913] Score: 4.82\n",
            "‚ùå [69_0_1771467603] Score: None\n",
            "‚ùå [74_0_1771468014] Score: 0.0\n",
            "‚ùå [74_1_1771468070] Score: None\n",
            "‚úÖ [15_0_1771464542] Score: 4.72\n",
            "‚úÖ [20_0_1771464833] Score: 4.83\n",
            "‚úÖ [20_1_1771464875] Score: 4.87\n",
            "‚ùå [43_0_1771466157] Score: None\n",
            "‚úÖ [51_0_1771466595] Score: 4.93\n",
            "‚úÖ [55_0_1771466806] Score: 4.86\n",
            "‚úÖ [55_1_1771466860] Score: 4.93\n",
            "‚ùå [57_0_1771466945] Score: 3.83\n",
            "‚úÖ [2_0_1771463740] Score: 4.86\n",
            "‚ùå [19_0_1771464799] Score: 3.83\n",
            "‚úÖ [42_0_1771466105] Score: 4.93\n",
            "‚úÖ [46_0_1771466304] Score: 4.93\n",
            "‚ùå [70_0_1771467662] Score: 3.87\n",
            "‚úÖ [70_1_1771467719] Score: 4.7\n",
            "‚ùå [24_0_1771465091] Score: None\n",
            "‚ùå [25_0_1771465140] Score: 0.0\n",
            "‚úÖ [71_0_1771467758] Score: 4.82\n",
            "‚úÖ [71_1_1771467818] Score: 4.67\n",
            "‚ùå [7_0_1771464060] Score: 0.0\n",
            "‚úÖ [7_1_1771464113] Score: 4.93\n",
            "‚úÖ [36_0_1771465719] Score: 4.65\n",
            "‚ùå [36_1_1771465761] Score: None\n",
            "‚ùå [37_0_1771465812] Score: None\n",
            "‚ùå [47_0_1771466339] Score: 0.0\n",
            "‚úÖ [50_0_1771466500] Score: 4.93\n",
            "‚úÖ [50_1_1771466541] Score: 4.93\n",
            "‚úÖ [9_0_1771464246] Score: 4.86\n",
            "‚úÖ [9_1_1771464285] Score: 4.82\n",
            "‚ùå [52_0_1771466631] Score: None\n",
            "‚ùå [52_1_1771466681] Score: 3.5\n",
            "‚úÖ Saved 120 audited records to vel_csv_transcripts_validation_001\n",
            "üèÅ Pipeline finished successfully.\n",
            "Validation execution time: 0:36:31.657655\n",
            "Upload execution time: 0:00:02.090526\n",
            "Total execution time: 0:36:33.748181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In average it takes ~20 seconds to validate a transcription"
      ],
      "metadata": {
        "id": "SB4W8BMaGt3w"
      },
      "id": "SB4W8BMaGt3w"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_results = pd.DataFrame(batch_results)\n",
        "print(\"Not Valid: \", df_results[df_results['is_valid'] == False].shape[0])\n",
        "print(\"Valid: \", df_results[df_results['is_valid'] == True].shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHkl7WZPMJn6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1771564344201,
          "user_tz": 360,
          "elapsed": 2,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "5906cf8c-23ab-4e79-a5a5-e8e3be8742ba"
      },
      "id": "CHkl7WZPMJn6",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not Valid:  47\n",
            "Valid:  73\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "vel_csv_transcription_validator_001",
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}