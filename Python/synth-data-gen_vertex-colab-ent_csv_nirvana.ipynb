{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Main Code"
      ],
      "metadata": {
        "id": "bjU7yc3ANkH_"
      },
      "id": "bjU7yc3ANkH_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Libraries"
      ],
      "metadata": {
        "id": "-nOP2Av89eWT"
      },
      "id": "-nOP2Av89eWT"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "id": "TvtEVTIykb2O",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1771530939476,
          "user_tz": 360,
          "elapsed": 8473,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40295dd-9067-49c2-d933-1db73628135f"
      },
      "id": "TvtEVTIykb2O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-40.4.0-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading faker-40.4.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.3/2.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-40.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import NotFound\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "from faker import Faker\n",
        "from typing import Tuple, List, Dict, Any"
      ],
      "metadata": {
        "id": "edbUQVlo9cqc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1771530945041,
          "user_tz": 360,
          "elapsed": 5362,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573c5dc4-26f4-408a-c911-db2cc8f30277"
      },
      "id": "edbUQVlo9cqc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.cloud.resourcemanager_v3 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.resourcemanager_v3 past that date.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.12) which Google will stop supporting in new releases of google.cloud.bigquery_storage_v1 once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.cloud.bigquery_storage_v1 past that date.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Configuration & Naming Standards"
      ],
      "metadata": {
        "id": "KKhEzA4C9hzJ"
      },
      "id": "KKhEzA4C9hzJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. CONFIGURATION & NAMING STANDARDS\n",
        "# ==========================================\n",
        "\n",
        "PROJECT_ID = \"project-nirvana-405904\"  # <--- REPLACE THIS\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "PROJECT_TAG = \"csv\"\n",
        "SCRIPT_VERSION = \"005\"\n",
        "TABLE_VERSION = \"005\"\n",
        "\n",
        "JSONL_FILE = f\"vel_{PROJECT_TAG}_transcripts_{SCRIPT_VERSION}.jsonl\"\n",
        "DATASET_ID = f\"vel_{PROJECT_TAG}_schema\"\n",
        "TABLE_ID = f\"vel_{PROJECT_TAG}_synthetic_transcripts_{TABLE_VERSION}\"\n",
        "\n",
        "# Initialize Vertex AI\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "model = GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "# Initialize Faker\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "3m1YdXlR9iNx"
      },
      "id": "3m1YdXlR9iNx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data Definitions (Personas, Products, Chaos)"
      ],
      "metadata": {
        "id": "JZkNBYaT9m8n"
      },
      "id": "JZkNBYaT9m8n"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. DATA DEFINITIONS (Personas, Products, Chaos)\n",
        "# ==========================================\n",
        "\n",
        "PERSONAS = {\n",
        "    \"Legacy Enterprise\": {\n",
        "        \"description\": \"Established 7+ years ago. High subs (2M-10M) but declining views.\",\n",
        "        \"pain_points\": [\"Subscribers not getting notifications\", \"Fear of irrelevance\"],\n",
        "        \"metrics\": {\"subs_range\": (2000000, 10000000), \"trend\": \"Declining\"},\n",
        "        \"spm_goal\": \"Strategic Pivot (modernization, community posts)\",\n",
        "        \"tone\": \"Professional, concerned, nostalgic\"\n",
        "    },\n",
        "    \"Short-Form Specialist\": {\n",
        "        \"description\": \"Rapid growth via Shorts (500k-5M subs). High views, low revenue.\",\n",
        "        \"pain_points\": [\"Monetization gap (low RPM)\", \"Brand safety\"],\n",
        "        \"metrics\": {\"subs_range\": (500000, 5000000), \"trend\": \"Explosive\"},\n",
        "        \"spm_goal\": \"Diversification (long-form, sponsorships)\",\n",
        "        \"tone\": \"Energetic, impatient, volume-focused\"\n",
        "    },\n",
        "    \"Niche Professional\": {\n",
        "        \"description\": \"High-value verticals (Finance, Tech). Lower views but high RPM.\",\n",
        "        \"pain_points\": [\"Limited Ads (Yellow Icon)\", \"Feature access\"],\n",
        "        \"metrics\": {\"subs_range\": (100000, 500000), \"trend\": \"Steady\"},\n",
        "        \"spm_goal\": \"Product Adoption (Shopping, Memberships)\",\n",
        "        \"tone\": \"Transactional, data-driven, calm\"\n",
        "    },\n",
        "    \"Viral Newcomer\": {\n",
        "        \"description\": \"Sudden massive spike. Lacks infrastructure. Overwhelmed.\",\n",
        "        \"pain_points\": [\"Identity Verification\", \"Copyright Claims\"],\n",
        "        \"metrics\": {\"subs_range\": (10000, 200000), \"trend\": \"Viral\"},\n",
        "        \"spm_goal\": \"Operational Health (Policy basics, Security)\",\n",
        "        \"tone\": \"Excited, chaotic, anxious, informal\"\n",
        "    }\n",
        "}\n",
        "\n",
        "PRODUCT_ECOSYSTEM = {\n",
        "    \"Monetization\": [\"AdSense RPM vs CPM\", \"Channel Memberships setup\", \"Super Chat strategies\", \"YouTube Shopping tagging\", \"Shorts Ad Revenue Sharing model\", \"YouTube BrandConnect campaigns\", \"Super Thanks activation & engagement\", \"YouTube Premium revenue calculation\"],\n",
        "    \"Content Formats\": [\"Shorts frequency (3x/week)\", \"Vertical Live discovery\", \"Podcast playlist setup\", \"Multi-Language Audio (Dubbing)\", \"YouTube Premieres (Trailers & Live Redirect)\", \"Community Tab strategy (Polls & Quizzes)\", \"Shorts Collab & Green Screen features\", \"Video Chapters (Manual vs. Automatic)\"],\n",
        "    \"Tools & Policy\": [\"Copyright Match Tool\", \"Yellow Icon (Limited Ads) appeals\", \"Audience Retention graph analysis\", \"Ad-friendly guidelines Self-Certification\", \"Content ID claims vs. Copyright Strikes\", \"Community Guidelines warning resolution training\", \"YouTube Analytics Research Tab (Search Insights)\"],\n",
        "    \"Events & Milestones\": [\"Gold Play Button delivery\", \"Fan Meetup security\", \"YouTube Creator Summits invitations\", \"YouTube FanFest participation\", \"Diamond Creator Award (10M) verification\"]\n",
        "}\n",
        "\n",
        "SCENARIOS = [\n",
        "    {\"type\": \"Quarterly Business Review\", \"duration_minutes\": 60, \"focus\": \"Strategic planning\", \"chaos_probability\": 0.2},\n",
        "    {\"type\": \"Annual Strategy Workshop\", \"duration_minutes\": 90, \"focus\": \"Long-term growth & multiple product adoption\", \"chaos_probability\": 0.1},\n",
        "    {\"type\": \"Crisis & Channel Audit\", \"duration_minutes\": 45, \"focus\": \"Urgent issue resolution + Health Check\", \"chaos_probability\": 0.6},\n",
        "    {\"type\": \"Content Strategy Deep Dive\", \"duration_minutes\": 120, \"focus\": \"Comprehensive content overhaul\", \"chaos_probability\": 0.3}\n",
        "]\n",
        "\n",
        "CHAOS_INSTRUCTIONS = {\n",
        "    \"Technical Glitch\": \"The Creator's audio crackles. They have to repeat themselves. The SPM sounds slightly annoyed but hides it.\",\n",
        "    \"Interruption\": \"A loud background noise (dog, doorbell) forces a 2-turn pause. The conversation loses its momentum.\",\n",
        "    \"Jargon Misunderstanding\": \"The Creator thinks 'RPM' is 'Revenue Per Million' and gets excited; the SPM must correct the math.\",\n",
        "    \"Tangent\": \"The Creator spends 3 turns complaining about a recent movie or the weather. The SPM has to struggle to bring the conversation back to business.\",\n",
        "    \"Budget Stress\": \"The Creator focuses obsessively on 'how much this costs' and ignores the security/policy benefits for several turns.\",\n",
        "    \"None\": \"Standard professional flow with minimal verbal fillers.\"\n",
        "}\n",
        "\n",
        "SPM_NAMES = [fake.name() for _ in range(20)]\n",
        "NICHES = [\"Gaming\", \"Beauty\", \"Tech Review\", \"Finance\", \"Vlog/Lifestyle\", \"Cooking\"]\n",
        "\n",
        "CHANNEL_PREFIXES = {\n",
        "    \"Gaming\": [\"Pixel\", \"Neon\", \"Retro\", \"Speed\", \"Shadow\", \"Elite\", \"Pro\", \"Quest\", \"Cyber\", \"Glitch\"],\n",
        "    \"Beauty\": [\"Glow\", \"Pure\", \"Luxe\", \"Velvet\", \"Chic\", \"Radiant\", \"Bella\", \"Silk\", \"Trend\", \"Glam\"],\n",
        "    \"Tech Review\": [\"Future\", \"Smart\", \"Tech\", \"Binary\", \"Gadget\", \"Silicon\", \"Digital\", \"Byte\", \"Hardware\", \"Logic\"],\n",
        "    \"Finance\": [\"Wealth\", \"Market\", \"Crypto\", \"Asset\", \"Value\", \"Capital\", \"Bull\", \"Fiscal\", \"Invest\", \"Money\"],\n",
        "    \"Vlog/Lifestyle\": [\"Daily\", \"Urban\", \"Wild\", \"Simple\", \"Happy\", \"Travel\", \"LifeWith\", \"Vibe\", \"Core\", \"Just\"],\n",
        "    \"Cooking\": [\"Tasty\", \"Chef\", \"Golden\", \"Spicy\", \"Fresh\", \"Yummy\", \"Kitchen\", \"Baked\", \"Savory\", \"Sweet\"]\n",
        "}\n",
        "\n",
        "CHANNEL_SUFFIXES = {\n",
        "    \"Gaming\": [\"Plays\", \"Gaming\", \"Arcade\", \"Zone\", \"Quest\", \"TV\", \"Live\", \"Station\", \"Hub\", \"Verse\"],\n",
        "    \"Beauty\": [\"Beauty\", \"Cosmetics\", \"Skin\", \"Style\", \"Makeup\", \"Looks\", \"Secrets\", \"Studio\", \"Room\", \"Diaries\"],\n",
        "    \"Tech Review\": [\"Reviews\", \"Lab\", \"Unboxed\", \"Flow\", \"Hub\", \"Central\", \"Insights\", \"Talks\", \"Breakdown\", \"Zone\"],\n",
        "    \"Finance\": [\"Watch\", \"Flow\", \"Capital\", \"Sense\", \"Moves\", \"Tips\", \"Guru\", \"Strategies\", \"Roadmap\", \"Hustle\"],\n",
        "    \"Vlog/Lifestyle\": [\"Vlogs\", \"Life\", \"Journeys\", \"Adventures\", \"Stories\", \"Days\", \"Moments\", \"World\", \"Lens\", \"Focus\"],\n",
        "    \"Cooking\": [\"Kitchen\", \"Eats\", \"Bites\", \"Table\", \"Recipes\", \"Bakery\", \"Cooks\", \"Flavors\", \"Delights\", \"Spot\"]\n",
        "}\n",
        "\n",
        "REGIONS = {\n",
        "    \"North America (USA)\": {\n",
        "        \"code\": \"en-US\",\n",
        "        \"style\": \"Direct, energetic, uses American idioms (bucks, awesome, dude).\",\n",
        "        \"cultural_context\": \"Western business casual.\"\n",
        "    },\n",
        "    \"Europe (UK)\": {\n",
        "        \"code\": \"en-GB\",\n",
        "        \"style\": \"Polite, perhaps slightly dry humor, uses British terms (cheers, mate, brilliant).\",\n",
        "        \"cultural_context\": \"European formality mixed with wit.\"\n",
        "    },\n",
        "    \"Asia Pacific (India)\": {\n",
        "        \"code\": \"en-IN\",\n",
        "        \"style\": \"Respectful, expressive, formal but warm, uses specific Indian English phrasing.\",\n",
        "        \"cultural_context\": \"High deference to authority/policy.\"\n",
        "    },\n",
        "    \"Latin America (Brazil)\": {\n",
        "        \"code\": \"en-BR\",\n",
        "        \"style\": \"Warm, engaging, enthusiastic, speaks English with a slight Portuguese cadence or phrasing.\",\n",
        "        \"cultural_context\": \"Relationship-focused.\"\n",
        "    },\n",
        "    \"Europe (Germany)\": {\n",
        "        \"code\": \"en-DE\",\n",
        "        \"style\": \"Direct, precise, efficient, less small talk, focuses on facts.\",\n",
        "        \"cultural_context\": \"Efficiency-focused.\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "rZG3lSHy9nL-"
      },
      "id": "rZG3lSHy9nL-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Helper Functions"
      ],
      "metadata": {
        "id": "UjF1In8u9qAM"
      },
      "id": "UjF1In8u9qAM"
    },
    {
      "cell_type": "code",
      "id": "niFtBbXQkiRSCB49okHSalaD",
      "metadata": {
        "tags": [],
        "id": "niFtBbXQkiRSCB49okHSalaD"
      },
      "source": [
        "# ==========================================\n",
        "# 3. HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def parse_raw_transcript_to_json(raw_text: str, start_time: datetime, expected_duration_minutes: int = 15) -> Tuple[List[Dict[str, Any]], str]:\n",
        "    \"\"\"\n",
        "    Parse JSON array produced by the generator into the structured transcript.\n",
        "    Includes DEFENSIVE LOGIC to handle 'Split-Object' hallucinations and 'Double-Escaping'.\n",
        "    \"\"\"\n",
        "    structured_transcript: List[Dict[str, Any]] = []\n",
        "    dialogue_list = []\n",
        "\n",
        "    try:\n",
        "        # 1. Cleaning Step: Handle Double Escaping (CSV style \"\"key\"\")\n",
        "        clean_text = raw_text\n",
        "        clean_text = re.sub(r\"^```(?:json)?\\s*\", \"\", clean_text)\n",
        "        clean_text = re.sub(r\"\\s*```$\", \"\", clean_text)\n",
        "\n",
        "        # Fix double quotes on keys if present (e.g. \"\"role\"\" -> \"role\")\n",
        "        # Be careful not to break content that legitimately has quotes.\n",
        "        # This regex targets double-quoted keys specifically.\n",
        "        clean_text = re.sub(r'\"\"(role|content)\"\"', r'\"\\1\"', clean_text)\n",
        "\n",
        "        dialogue_list_raw = json.loads(clean_text)\n",
        "\n",
        "        if not isinstance(dialogue_list_raw, list):\n",
        "            raise ValueError(\"Parsed JSON is not an array.\")\n",
        "\n",
        "        # 2. Repair Logic: Handle Split-Object Hallucination\n",
        "        # Symptom: [{\"role\": \"role\", \"content\": \"SPM\"}, {\"role\": \"content\", \"content\": \"text...\"}]\n",
        "\n",
        "        current_speaker = None\n",
        "\n",
        "        for item in dialogue_list_raw:\n",
        "            # Check if this item is a valid, complete turn\n",
        "            if \"role\" in item and \"content\" in item:\n",
        "\n",
        "                # CASE A: Standard Valid Item\n",
        "                # If role is SPM or Creator, and content looks like a message\n",
        "                if item[\"role\"] in [\"SPM\", \"Creator\", \"Action\"] and item[\"role\"] != \"role\":\n",
        "                    dialogue_list.append(item)\n",
        "                    current_speaker = None # Reset\n",
        "                    continue\n",
        "\n",
        "                # CASE B: The \"Split-Object\" Defect\n",
        "                # Sub-case B1: The item defines the ROLE (e.g. role=\"role\", content=\"SPM\")\n",
        "                if item[\"role\"] == \"role\" or item.get(\"content\") in [\"SPM\", \"Creator\"]:\n",
        "                    # We found the speaker, store it for the next iteration\n",
        "                    current_speaker = item.get(\"content\")\n",
        "                    if current_speaker not in [\"SPM\", \"Creator\", \"Action\"]:\n",
        "                        current_speaker = None # Invalid speaker value\n",
        "\n",
        "                # Sub-case B2: The item defines the CONTENT (e.g. role=\"content\", content=\"Hello\")\n",
        "                elif item[\"role\"] == \"content\" and current_speaker:\n",
        "                    # We have a speaker waiting from previous loop, and now we have content\n",
        "                    dialogue_list.append({\n",
        "                        \"role\": current_speaker,\n",
        "                        \"content\": item[\"content\"]\n",
        "                    })\n",
        "                    current_speaker = None # Consumed\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è JSON Parsing Error (Fallback active): {e}\")\n",
        "        # Fallback: Regex extraction\n",
        "        dialogue_list = []\n",
        "        for line in raw_text.splitlines():\n",
        "            m = re.match(r'^\\s*(SPM|Creator)\\s*[:\\-]\\s*(.+)', line)\n",
        "            if m:\n",
        "                dialogue_list.append({\"role\": m.group(1), \"content\": m.group(2).strip()})\n",
        "        if not dialogue_list:\n",
        "            return [], start_time.isoformat()\n",
        "\n",
        "    # 3. Standard Processing (Timestamping)\n",
        "    # If repair resulted in empty list, return empty\n",
        "    if not dialogue_list:\n",
        "         return [], start_time.isoformat()\n",
        "\n",
        "    total_msgs = max(1, len(dialogue_list))\n",
        "    total_seconds = max(1, int(expected_duration_minutes * 60))\n",
        "\n",
        "    for idx, entry in enumerate(dialogue_list):\n",
        "        role = entry.get(\"role\")\n",
        "        content = entry.get(\"content\", \"\")\n",
        "\n",
        "        # Normalize speaker code for BigQuery/Downstream\n",
        "        if role == \"SPM\":\n",
        "            speaker_code = \"A\"\n",
        "        elif role == \"Creator\":\n",
        "            speaker_code = \"B\"\n",
        "        else:\n",
        "            speaker_code = \"Action\"\n",
        "\n",
        "        frac = idx / (total_msgs - 1) if total_msgs > 1 else 0\n",
        "        base_seconds = int(frac * total_seconds)\n",
        "        jitter = random.randint(-5, 10)\n",
        "        msg_time = start_time + timedelta(seconds=max(0, base_seconds + jitter))\n",
        "\n",
        "        structured_transcript.append({\n",
        "            \"speaker\": speaker_code,\n",
        "            \"timestamp\": msg_time.isoformat(),\n",
        "            \"text\": content\n",
        "        })\n",
        "\n",
        "    end_time = start_time + timedelta(seconds=total_seconds + random.randint(0, 10))\n",
        "    return structured_transcript, end_time.isoformat()\n",
        "\n",
        "\n",
        "def normalize_record_for_jsonl(rec: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Clean up types for BigQuery ingestion.\n",
        "    Simplified to remove validation specific fields.\n",
        "    \"\"\"\n",
        "    normalized = rec.copy()\n",
        "\n",
        "    for k in (\"recording_start\", \"recording_end\"):\n",
        "        v = normalized.get(k)\n",
        "        if isinstance(v, datetime):\n",
        "            normalized[k] = v.isoformat()\n",
        "        else:\n",
        "            try:\n",
        "                datetime.fromisoformat(v)\n",
        "                normalized[k] = v\n",
        "            except Exception:\n",
        "                normalized[k] = \"\"\n",
        "\n",
        "    try:\n",
        "        normalized[\"duration_minutes\"] = float(normalized.get(\"duration_minutes\", 0.0))\n",
        "    except Exception:\n",
        "        normalized[\"duration_minutes\"] = 0.0\n",
        "\n",
        "    return normalized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Generation Engine"
      ],
      "metadata": {
        "id": "_drXH0CD9s-b"
      },
      "id": "_drXH0CD9s-b"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. GENERATION ENGINE\n",
        "# ==========================================\n",
        "\n",
        "class CreatorAgent:\n",
        "    def __init__(self, id: int, persona_name: str):\n",
        "        self.id = id\n",
        "        self.persona_name = persona_name\n",
        "        self.persona_data = PERSONAS[persona_name]\n",
        "        self.niche = random.choice(NICHES)\n",
        "        self.experience_level = self.persona_data.get(\"experience_level\", \"intermediate\")\n",
        "        self.subs = random.randint(self.persona_data['metrics']['subs_range'][0], self.persona_data['metrics']['subs_range'][1])\n",
        "\n",
        "        self.region_name = random.choice(list(REGIONS.keys()))\n",
        "        self.region_data = REGIONS[self.region_name]\n",
        "\n",
        "        prefix = random.choice(CHANNEL_PREFIXES[self.niche])\n",
        "        suffix = random.choice(CHANNEL_SUFFIXES[self.niche])\n",
        "        unique_tail = str(random.randint(1, 999)) if random.random() < 0.6 else fake.word().capitalize()\n",
        "        self.channel_name = f\"{prefix}{suffix}{unique_tail}\"\n",
        "\n",
        "        self.creator_display_name = fake.name()\n",
        "        self.assigned_spm = random.choice(SPM_NAMES) # We don't use faker on this to have a SPM with several creators\n",
        "        self.history: List[str] = []\n",
        "\n",
        "class SimulationEngine:\n",
        "    def __init__(self, target_rows=10):\n",
        "        self.target_rows = target_rows\n",
        "        self.transcript_db = []\n",
        "\n",
        "    # HELPER: Flatten topics for easy selection\n",
        "    def get_all_topics(self):\n",
        "        all_topics = []\n",
        "        for category, topics in PRODUCT_ECOSYSTEM.items():\n",
        "            all_topics.extend(topics)\n",
        "        return all_topics\n",
        "\n",
        "    def generate_system_prompt(self, creator, scenario, chaos_key, topic_list, is_followup):\n",
        "        history_context = f\"PREVIOUS CONTEXT: {creator.history[-1]}\" if is_followup and creator.history else \"This is the first call.\"\n",
        "        chaos_instruction = CHAOS_INSTRUCTIONS.get(chaos_key, \"No interruptions.\")\n",
        "        duration = scenario.get('duration_minutes', 60)\n",
        "\n",
        "        # Calculate turns based on duration (approx 1.2 turns per minute for long calls to stay within token limits)\n",
        "        target_turns = int(duration * 1.2)\n",
        "\n",
        "        # Format the list of topics into a bulleted string\n",
        "        agenda_items = \"\\n\".join([f\"   - {t}\" for t in topic_list])\n",
        "\n",
        "        # Dynamic Structure Guide based on DURATION\n",
        "        if duration >= 90: # Very Long Call (1.5 - 2 hours)\n",
        "            structure_guide = f\"\"\"\n",
        "            **MANDATORY STRUCTURE (DEEP DIVE CALL - {duration} mins):**\n",
        "            1. [0-10 min] Warm-up, relationship building, and catching up on personal life/culture.\n",
        "            2. [10-20 min] Channel Health Check: Reviewing analytics (Views, RPM, Subscribers).\n",
        "            3. [20-80 min] **THE AGENDA (Core Discussion):** Discuss the following topics sequentially. Allow natural transitions between them:\n",
        "            {agenda_items}\n",
        "            4. [80-100 min] Brainstorming session for future content ideas based on these topics.\n",
        "            5. [100-{duration} min] Defining clear action items and closing.\n",
        "            \"\"\"\n",
        "        elif duration >= 45: # Standard QBR (45-60 mins)\n",
        "            structure_guide = f\"\"\"\n",
        "            **MANDATORY STRUCTURE (STRATEGY CALL - {duration} mins):**\n",
        "            1. [0-5 min] Intro and check-in.\n",
        "            2. [5-15 min] Performance Review.\n",
        "            3. [15-{duration-10} min] **Main Topics:** Cover these points in detail:\n",
        "            {agenda_items}\n",
        "            4. [{duration-10}-{duration} min] Q&A and Closing.\n",
        "            \"\"\"\n",
        "        else: # Short Call (Crisis)\n",
        "            structure_guide = f\"\"\"\n",
        "            **MANDATORY STRUCTURE (SHORT/CRISIS CALL - {duration} mins):**\n",
        "            1. [0-5 min] Immediate Triage: Address the main crisis ({topic_list[0]}).\n",
        "            2. [5-{duration-5} min] Brief check on secondary topics if time permits:\n",
        "            {agenda_items}\n",
        "            3. [{duration-5}-{duration} min] Wrap up.\n",
        "            \"\"\"\n",
        "\n",
        "        realism_guidelines = \"\"\"\n",
        "        HUMAN REALISM GUIDELINES:\n",
        "        - Add false starts, partial overlaps, self-interruptions.\n",
        "        - Include jargon misunderstanding.\n",
        "        - Allow topic drift.\n",
        "        - Make it feel recorded live, not scripted.\n",
        "        \"\"\"\n",
        "\n",
        "        # BLOCK: JSON INTEGRITY RULES\n",
        "        # Explicitly forbids the split-object hallucination observed in the incident report.\n",
        "        json_integrity_rules = \"\"\"\n",
        "        **CRITICAL JSON FORMATTING RULES (STRICT ENFORCEMENT):**\n",
        "        1. **NO SPLIT OBJECTS:** Do NOT separate the role and the content into different objects.\n",
        "           - ‚ùå INCORRECT: [{\"role\": \"role\", \"content\": \"SPM\"}, {\"role\": \"content\", \"content\": \"Hello\"}]\n",
        "           - ‚úÖ CORRECT:   [{\"role\": \"SPM\", \"content\": \"Hello\"}]\n",
        "        2. **NO DOUBLE QUOTES ON KEYS:** Do not use CSV-style double escaping.\n",
        "           - ‚ùå INCORRECT: [{\"\"role\"\": \"\"SPM\"\"}]\n",
        "           - ‚úÖ CORRECT:   [{\"role\": \"SPM\"}]\n",
        "        3. **SINGLE OBJECT PER TURN:** Each list item must contain BOTH \"role\" and \"content\".\n",
        "        \"\"\"\n",
        "\n",
        "        return f\"\"\"\n",
        "        Generate a realistic, verbatim call transcript between a YouTube SPM and a Creator.\n",
        "\n",
        "        {realism_guidelines}\n",
        "\n",
        "        **ROLE 1: SPM ({creator.assigned_spm})**\n",
        "        - Goal: Cover the full agenda of {len(topic_list)} topics while maintaining the relationship.\n",
        "        - Tone: Professional, uses jargon (YPP, CTR), empathetic.\n",
        "\n",
        "        **ROLE 2: CREATOR ({creator.channel_name})**\n",
        "        - Persona: {creator.persona_name} ({creator.persona_data['tone']}).\n",
        "        - Region: {creator.region_name} ({creator.region_data['style']})\n",
        "        - Pain Point: {creator.persona_data['pain_points'][0]}.\n",
        "\n",
        "        **SCENARIO:** {scenario['type']} ({duration} minutes).\n",
        "        **CHAOS:** {chaos_instruction}\n",
        "        **CONTEXT:** {history_context}\n",
        "\n",
        "        **TOPICS TO DISCUSS (AGENDA):**\n",
        "        {agenda_items}\n",
        "\n",
        "        {structure_guide}\n",
        "\n",
        "        {json_integrity_rules}\n",
        "\n",
        "        **INSTRUCTIONS:**\n",
        "        1. **LENGTH:** At least **{target_turns} dialogue turns**. DO NOT SUMMARIZE.\n",
        "        2. **DEPTH:** You must touch upon ALL topics listed in the Agenda.\n",
        "        3. **FORMAT:** Output a VALID JSON ARRAY:\n",
        "           [\n",
        "             {{\"role\": \"SPM\", \"content\": \"...\"}},\n",
        "             {{\"role\": \"Creator\", \"content\": \"...\"}}\n",
        "           ]\n",
        "        \"\"\"\n",
        "\n",
        "    def run(self):\n",
        "        print(f\"üöÄ Starting generation. Target: {self.target_rows} rows...\")\n",
        "\n",
        "        rows_generated = 0\n",
        "        creator_id_counter = 1\n",
        "        all_possible_topics = self.get_all_topics() # Get flat list of topics\n",
        "\n",
        "        while rows_generated < self.target_rows:\n",
        "            creator = CreatorAgent(creator_id_counter, random.choice(list(PERSONAS.keys())))\n",
        "            creator_id_counter += 1\n",
        "            num_calls = random.choices([1, 2, 3, 4], weights=[50, 30, 15, 5], k=1)[0]\n",
        "\n",
        "            print(f\"Processing {creator.channel_name} ({creator.region_name}) - Planning {num_calls} call(s)...\")\n",
        "\n",
        "            for call_idx in range(num_calls):\n",
        "                if rows_generated >= self.target_rows: break\n",
        "\n",
        "                scenario = random.choice(SCENARIOS)\n",
        "                duration = scenario.get('duration_minutes', 60)\n",
        "                chaos_key = \"None\" if random.random() > scenario['chaos_probability'] else random.choice(list(CHAOS_INSTRUCTIONS.keys()))\n",
        "\n",
        "                # --- TOPIC SELECTION LOGIC (NEW) ---\n",
        "                # Determine how many topics based on duration\n",
        "                if duration >= 90:\n",
        "                    num_topics = random.randint(10, 18) # Deep dive: 8 to 15 topics\n",
        "                elif duration >= 45:\n",
        "                    num_topics = random.randint(6, 10)  # Standard: 4 to 8 topics\n",
        "                else:\n",
        "                    num_topics = random.randint(3, 5)  # Crisis: 1 to 3 topics\n",
        "\n",
        "                # Select random unique topics\n",
        "                selected_topics = random.sample(all_possible_topics, min(num_topics, len(all_possible_topics)))\n",
        "\n",
        "                # Primary topic for metadata (the first one)\n",
        "                primary_topic = selected_topics[0]\n",
        "                # Join all topics for metadata storage (optional, or just store list)\n",
        "                full_topic_string = \"; \".join(selected_topics)\n",
        "\n",
        "                start_dt = datetime.now() - timedelta(days=random.randint(1, 30))\n",
        "                conv_id = f\"{creator.id}_{call_idx}_{int(time.time())}\"\n",
        "\n",
        "                try:\n",
        "                    # 1. Generate Raw Transcript with MULTIPLE TOPICS\n",
        "                    prompt = self.generate_system_prompt(creator, scenario, chaos_key, selected_topics, call_idx > 0)\n",
        "\n",
        "                    response = model.generate_content(\n",
        "                        prompt,\n",
        "                        generation_config={\n",
        "                            \"temperature\": 0.7,\n",
        "                            \"response_mime_type\": \"application/json\",\n",
        "                            \"max_output_tokens\": 8192 # Crucial for long calls\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                    try:\n",
        "                        raw_transcript_json_string = response.text\n",
        "                    except ValueError:\n",
        "                        parts = response.candidates[0].content.parts\n",
        "                        raw_transcript_json_string = \"\".join([part.text for part in parts])\n",
        "\n",
        "                    # 2. Parse\n",
        "                    structured_transcript, real_end_time_iso = parse_raw_transcript_to_json(\n",
        "                        raw_transcript_json_string,\n",
        "                        start_dt,\n",
        "                        expected_duration_minutes=duration\n",
        "                    )\n",
        "\n",
        "                    # 3. Summarize\n",
        "                    summary = model.generate_content(f\"Summarize in 1 sentence: {raw_transcript_json_string[:2000]}\").text.strip()\n",
        "                    creator.history.append(summary)\n",
        "\n",
        "                    # 4. Calculate duration\n",
        "                    try:\n",
        "                        start_iso = start_dt\n",
        "                        end_iso = datetime.fromisoformat(real_end_time_iso)\n",
        "                        duration_minutes_real = round((end_iso - start_iso).total_seconds() / 60.0, 2)\n",
        "                    except Exception:\n",
        "                        duration_minutes_real = float(duration)\n",
        "\n",
        "                    # 5. Build Record\n",
        "                    record = {\n",
        "                        \"conversation_id\": conv_id,\n",
        "                        \"creator_id\": creator.id,\n",
        "                        \"channel_name\": creator.channel_name,\n",
        "                        \"creator_niche\": creator.niche,\n",
        "                        \"creator_persona\": creator.persona_name,\n",
        "                        \"creator_tone\": creator.persona_data['tone'],\n",
        "                        \"spm_name\": creator.assigned_spm,\n",
        "                        \"creator_region\": creator.region_name,\n",
        "                        \"language_code\": creator.region_data['code'],\n",
        "                        \"scenario\": scenario['type'],\n",
        "                        \"product_topic\": full_topic_string, # Store ALL topics here\n",
        "                        \"duration_minutes\": duration_minutes_real,\n",
        "                        \"recording_start\": start_dt.isoformat(),\n",
        "                        \"recording_end\": real_end_time_iso,\n",
        "                        \"raw_transcript\": raw_transcript_json_string\n",
        "                    }\n",
        "\n",
        "                    self.transcript_db.append(record)\n",
        "                    rows_generated += 1\n",
        "                    print(f\"  ‚úÖ Generated: {conv_id} ({len(selected_topics)} topics, ~{duration_minutes_real} mins)\")\n",
        "\n",
        "                    time.sleep(1)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing call {conv_id}: {e}\")\n",
        "\n",
        "        # Final Save\n",
        "        with open(JSONL_FILE, 'w', encoding='utf-8') as f:\n",
        "            for entry in self.transcript_db:\n",
        "                clean_entry = normalize_record_for_jsonl(entry)\n",
        "                f.write(json.dumps(clean_entry, ensure_ascii=False, separators=(',', ':')) + '\\n')\n",
        "        print(f\"‚úÖ Saved {rows_generated} transcripts to: {JSONL_FILE}\")"
      ],
      "metadata": {
        "id": "UQjfNoRr9tOj"
      },
      "id": "UQjfNoRr9tOj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. BigQuery Upload"
      ],
      "metadata": {
        "id": "NzyRo15Q9w1Z"
      },
      "id": "NzyRo15Q9w1Z"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. BIGQUERY UPLOAD\n",
        "# ==========================================\n",
        "\n",
        "def upload_to_bigquery():\n",
        "    client = bigquery.Client(project=PROJECT_ID)\n",
        "    dataset_ref = client.dataset(DATASET_ID)\n",
        "\n",
        "    try:\n",
        "        client.get_dataset(dataset_ref)\n",
        "    except NotFound:\n",
        "        dataset = bigquery.Dataset(dataset_ref)\n",
        "        dataset.location = \"US\"\n",
        "        client.create_dataset(dataset)\n",
        "        print(f\"‚úÖ Created dataset {DATASET_ID}\")\n",
        "\n",
        "    table_ref = dataset_ref.table(TABLE_ID)\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "    job_config.autodetect = True\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "\n",
        "    with open(JSONL_FILE, \"rb\") as source_file:\n",
        "        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
        "\n",
        "    job.result()\n",
        "    print(f\"üéâ Success! Loaded {job.output_rows} rows.\")"
      ],
      "metadata": {
        "id": "gb8_PkXH9xBo"
      },
      "id": "gb8_PkXH9xBo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Main"
      ],
      "metadata": {
        "id": "e4HKuRwCNt3c"
      },
      "id": "e4HKuRwCNt3c"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    # A. Create synthetic data\n",
        "    sim = SimulationEngine(target_rows=20)\n",
        "    sim.run()\n",
        "    end_time_sd = time.perf_counter()\n",
        "\n",
        "    # B. Upload to BigQuery\n",
        "    upload_to_bigquery()\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "    duration_sd = str(timedelta(seconds=end_time_sd - start_time))\n",
        "    duration_bq = str(timedelta(seconds=end_time - end_time_sd))\n",
        "    duration_total = str(timedelta(seconds=end_time - start_time))\n",
        "\n",
        "    print(f\"Synthetic Data execution time (HH:MM:SS): {duration_sd}\")\n",
        "    print(f\"Upload to BigQuery execution time (HH:MM:SS): {duration_bq}\")\n",
        "    print(f\"Total execution time (HH:MM:SS): {duration_total}\")"
      ],
      "metadata": {
        "id": "RZtiauz0Nwwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "33ffe87c-2477-467c-db99-69bbe64ae50a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1771531900655,
          "user_tz": 360,
          "elapsed": 955617,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "RZtiauz0Nwwg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting generation. Target: 20 rows...\n",
            "Processing SpeedZone105 (Asia Pacific (India)) - Planning 2 call(s)...\n",
            "  ‚úÖ Generated: 1_0_1771530949 (6 topics, ~60.0 mins)\n",
            "  ‚úÖ Generated: 1_1_1771530986 (7 topics, ~45.12 mins)\n",
            "Processing WildWorldAgreement (Europe (UK)) - Planning 1 call(s)...\n",
            "‚ö†Ô∏è JSON Parsing Error (Fallback active): Unterminated string starting at: line 367 column 13 (char 23816)\n",
            "  ‚úÖ Generated: 2_0_1771531023 (13 topics, ~0.0 mins)\n",
            "Processing JustAdventures725 (Europe (UK)) - Planning 1 call(s)...\n",
            "  ‚úÖ Generated: 3_0_1771531075 (8 topics, ~60.05 mins)\n",
            "Processing TrendBeautyTrue (Europe (Germany)) - Planning 1 call(s)...\n",
            "  ‚úÖ Generated: 4_0_1771531118 (13 topics, ~120.0 mins)\n",
            "Processing RadiantMakeupGrow (Latin America (Brazil)) - Planning 1 call(s)...\n",
            "‚ö†Ô∏è JSON Parsing Error (Fallback active): Unterminated string starting at: line 352 column 16 (char 25281)\n",
            "  ‚úÖ Generated: 5_0_1771531169 (13 topics, ~0.0 mins)\n",
            "Processing LogicBreakdown922 (Latin America (Brazil)) - Planning 1 call(s)...\n",
            "  ‚úÖ Generated: 6_0_1771531227 (6 topics, ~60.13 mins)\n",
            "Processing NeonStation476 (Europe (UK)) - Planning 2 call(s)...\n",
            "  ‚úÖ Generated: 7_0_1771531273 (9 topics, ~45.15 mins)\n",
            "  ‚úÖ Generated: 7_1_1771531308 (10 topics, ~60.1 mins)\n",
            "Processing GadgetTalks476 (North America (USA)) - Planning 2 call(s)...\n",
            "‚ö†Ô∏è JSON Parsing Error (Fallback active): Expecting property name enclosed in double quotes: line 304 column 1 (char 24834)\n",
            "  ‚úÖ Generated: 8_0_1771531350 (13 topics, ~0.0 mins)\n",
            "  ‚úÖ Generated: 8_1_1771531407 (13 topics, ~90.1 mins)\n",
            "Processing FreshKitchen724 (Latin America (Brazil)) - Planning 2 call(s)...\n",
            "‚ö†Ô∏è JSON Parsing Error (Fallback active): Unterminated string starting at: line 360 column 16 (char 21957)\n",
            "  ‚úÖ Generated: 9_0_1771531453 (13 topics, ~0.0 mins)\n",
            "  ‚úÖ Generated: 9_1_1771531512 (8 topics, ~45.13 mins)\n",
            "Processing QuestGamingValue (North America (USA)) - Planning 1 call(s)...\n",
            "‚ö†Ô∏è JSON Parsing Error (Fallback active): Expecting ':' delimiter: line 15 column 14 (char 688)\n",
            "  ‚úÖ Generated: 10_0_1771531556 (9 topics, ~0.0 mins)\n",
            "Processing EliteHub3 (Latin America (Brazil)) - Planning 2 call(s)...\n",
            "‚ö†Ô∏è JSON Parsing Error (Fallback active): Unterminated string starting at: line 432 column 5 (char 24323)\n",
            "  ‚úÖ Generated: 11_0_1771531592 (13 topics, ~0.0 mins)\n",
            "‚ö†Ô∏è JSON Parsing Error (Fallback active): Unterminated string starting at: line 292 column 5 (char 25271)\n",
            "  ‚úÖ Generated: 11_1_1771531648 (13 topics, ~0.0 mins)\n",
            "Processing ProArcade801 (Europe (UK)) - Planning 1 call(s)...\n",
            "‚ö†Ô∏è JSON Parsing Error (Fallback active): Expecting property name enclosed in double quotes: line 307 column 5 (char 26348)\n",
            "  ‚úÖ Generated: 12_0_1771531701 (13 topics, ~0.0 mins)\n",
            "Processing WealthWatch58 (Asia Pacific (India)) - Planning 1 call(s)...\n",
            "‚ö†Ô∏è JSON Parsing Error (Fallback active): Unterminated string starting at: line 352 column 16 (char 24661)\n",
            "  ‚úÖ Generated: 13_0_1771531757 (13 topics, ~0.0 mins)\n",
            "Processing FiscalMovesHospital (North America (USA)) - Planning 1 call(s)...\n",
            "  ‚úÖ Generated: 14_0_1771531814 (8 topics, ~45.15 mins)\n",
            "Processing SpicyFlavors906 (North America (USA)) - Planning 1 call(s)...\n",
            "  ‚úÖ Generated: 15_0_1771531852 (13 topics, ~90.17 mins)\n",
            "‚úÖ Saved 20 transcripts to: vel_csv_transcripts_005.jsonl\n",
            "üéâ Success! Loaded 20 rows.\n",
            "Synthetic Data execution time (HH:MM:SS): 0:15:51.968180\n",
            "Upload to BigQuery execution time (HH:MM:SS): 0:00:03.607960\n",
            "Total execution time (HH:MM:SS): 0:15:55.576140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In average it takes ~36 seconds to create a transcription"
      ],
      "metadata": {
        "id": "r09m4JqmGk88"
      },
      "id": "r09m4JqmGk88"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "vel_csv_synthetic_data_generator_005",
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
