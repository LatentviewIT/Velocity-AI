{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Main Code"
      ],
      "metadata": {
        "id": "bjU7yc3ANkH_"
      },
      "id": "bjU7yc3ANkH_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Libraries"
      ],
      "metadata": {
        "id": "-nOP2Av89eWT"
      },
      "id": "-nOP2Av89eWT"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "id": "TvtEVTIykb2O"
      },
      "id": "TvtEVTIykb2O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import NotFound, ResourceExhausted\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import uuid\n",
        "from datetime import datetime, timedelta\n",
        "from faker import Faker\n",
        "from typing import Tuple, List, Dict, Any"
      ],
      "metadata": {
        "id": "edbUQVlo9cqc"
      },
      "id": "edbUQVlo9cqc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Configuration & Naming Standards"
      ],
      "metadata": {
        "id": "KKhEzA4C9hzJ"
      },
      "id": "KKhEzA4C9hzJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. CONFIGURATION & NAMING STANDARDS\n",
        "# ==========================================\n",
        "\n",
        "PROJECT_ID = \"project-nirvana-405904\"  # <--- REPLACE THIS\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "PROJECT_TAG = \"csv\"\n",
        "SCRIPT_VERSION = \"006\"\n",
        "TABLE_VERSION = \"006\"\n",
        "\n",
        "JSONL_FILE = f\"vel_{PROJECT_TAG}_transcripts_{SCRIPT_VERSION}.jsonl\"\n",
        "DATASET_ID = f\"vel_{PROJECT_TAG}_schema\"\n",
        "TABLE_ID = f\"vel_{PROJECT_TAG}_synthetic_transcripts_{TABLE_VERSION}\"\n",
        "\n",
        "# Initialize Vertex AI\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "model = GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "# Initialize Faker\n",
        "fake = Faker()\n",
        "\n",
        "# With seed the 1¬∞ conversation generated will have the same metadata as other days (just ID's and raw_transcriptions will be different).\n",
        "# So, we commented these lines to avoid \"similar\" transcriptions metadata (You could found a same creator_name, topics, region, etc. with different creator_id).\n",
        "#Faker.seed(42)\n",
        "#random.seed(42)"
      ],
      "metadata": {
        "id": "3m1YdXlR9iNx"
      },
      "id": "3m1YdXlR9iNx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data Definitions (Personas, Products, Chaos)"
      ],
      "metadata": {
        "id": "JZkNBYaT9m8n"
      },
      "id": "JZkNBYaT9m8n"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. DATA DEFINITIONS (Personas, Products, Chaos)\n",
        "# ==========================================\n",
        "\n",
        "PERSONAS = {\n",
        "    \"Legacy Enterprise\": {\n",
        "        \"description\": \"Established 7+ years ago. High subs (2M-10M) but declining views.\",\n",
        "        \"pain_points\": [\"Subscribers not getting notifications\", \"Fear of irrelevance\"],\n",
        "        \"metrics\": {\"subs_range\": (2000000, 10000000), \"trend\": \"Declining\"},\n",
        "        \"spm_goal\": \"Strategic Pivot (modernization, community posts)\",\n",
        "        \"tone\": \"Professional, concerned, nostalgic\"\n",
        "    },\n",
        "    \"Short-Form Specialist\": {\n",
        "        \"description\": \"Rapid growth via Shorts (500k-5M subs). High views, low revenue.\",\n",
        "        \"pain_points\": [\"Monetization gap (low RPM)\", \"Brand safety\"],\n",
        "        \"metrics\": {\"subs_range\": (500000, 5000000), \"trend\": \"Explosive\"},\n",
        "        \"spm_goal\": \"Diversification (long-form, sponsorships)\",\n",
        "        \"tone\": \"Energetic, impatient, volume-focused\"\n",
        "    },\n",
        "    \"Niche Professional\": {\n",
        "        \"description\": \"High-value verticals (Finance, Tech). Lower views but high RPM.\",\n",
        "        \"pain_points\": [\"Limited Ads (Yellow Icon)\", \"Feature access\"],\n",
        "        \"metrics\": {\"subs_range\": (100000, 500000), \"trend\": \"Steady\"},\n",
        "        \"spm_goal\": \"Product Adoption (Shopping, Memberships)\",\n",
        "        \"tone\": \"Transactional, data-driven, calm\"\n",
        "    },\n",
        "    \"Viral Newcomer\": {\n",
        "        \"description\": \"Sudden massive spike. Lacks infrastructure. Overwhelmed.\",\n",
        "        \"pain_points\": [\"Identity Verification\", \"Copyright Claims\"],\n",
        "        \"metrics\": {\"subs_range\": (10000, 200000), \"trend\": \"Viral\"},\n",
        "        \"spm_goal\": \"Operational Health (Policy basics, Security)\",\n",
        "        \"tone\": \"Excited, chaotic, anxious, informal\"\n",
        "    },\n",
        "    \"Cross-Platform Hustler\": {\n",
        "        \"description\": \"Massive following on TikTok/Twitch, trying to port success to YouTube.\",\n",
        "        \"pain_points\": [\"Algorithm differences\", \"Conversion from Shorts to Long-form\"],\n",
        "        \"metrics\": {\"subs_range\": (100000, 1000000), \"trend\": \"Growing\"},\n",
        "        \"spm_goal\": \"Platform Loyalty (Live Streaming, Premieres)\",\n",
        "        \"tone\": \"Business-focused, impatient, constantly comparing platforms\"\n",
        "    },\n",
        "    \"Burned-out Veteran\": {\n",
        "        \"description\": \"Highly successful but mentally exhausted from the upload grind.\",\n",
        "        \"pain_points\": [\"Upload Cadence Stress\", \"Team Scaling\"],\n",
        "        \"metrics\": {\"subs_range\": (1000000, 5000000), \"trend\": \"Stagnant\"},\n",
        "        \"spm_goal\": \"Sustainability (Evergreen content, pausing without penalty)\",\n",
        "        \"tone\": \"Exhausted, cynical but open to help, overwhelmed\"\n",
        "    }\n",
        "}\n",
        "\n",
        "TOPICS_CATALOG = [\n",
        "    # Monetization\n",
        "    \"[Monetization > Ad Revenue Optimization] RPM\",\n",
        "    \"[Monetization > Ad Revenue Optimization] CPM\",\n",
        "    \"[Monetization > Ad Revenue Optimization] Geo Mix\",\n",
        "    \"[Monetization > Ad Revenue Optimization] Seasonality\",\n",
        "    \"[Monetization > Ad Revenue Optimization] Long Form vs Shorts\",\n",
        "    \"[Monetization > Fan Funding Optimization] Channel Memberships\",\n",
        "    \"[Monetization > Fan Funding Optimization] Super Chats\",\n",
        "    \"[Monetization > Fan Funding Optimization] Super Thanks\",\n",
        "    \"[Monetization > Fan Funding Optimization] Recurring Revenue\",\n",
        "    \"[Monetization > Fan Funding Optimization] Churn\",\n",
        "    \"[Monetization > Commerce Optimization] Shopping\",\n",
        "    \"[Monetization > Commerce Optimization] Affiliate\",\n",
        "    \"[Monetization > Commerce Optimization] Product Tagging\",\n",
        "    \"[Monetization > Commerce Optimization] Conversion\",\n",
        "    \"[Monetization > Brand Revenue Strategy] Brand Deals\",\n",
        "    \"[Monetization > Brand Revenue Strategy] Brand Connect\",\n",
        "    \"[Monetization > Brand Revenue Strategy] Sponsor Integrations\",\n",
        "    \"[Monetization > Brand Revenue Strategy] Platform Ads\",\n",
        "\n",
        "    # Content & Formats\n",
        "    \"[Content & Formats > Shorts Strategy] Shorts growth vs revenue\",\n",
        "    \"[Content & Formats > Shorts Strategy] Shorts Collab\",\n",
        "    \"[Content & Formats > Shorts Strategy] Shorts Experimentation\",\n",
        "    \"[Content & Formats > Live and Event Strategy] Live Streaming\",\n",
        "    \"[Content & Formats > Live and Event Strategy] Premieres\",\n",
        "    \"[Content & Formats > Live and Event Strategy] Redirect Strategy\",\n",
        "    \"[Content & Formats > Live and Event Strategy] Real-Time Monetization\",\n",
        "    \"[Content & Formats > Live and Event Strategy] Scheduled Launches\",\n",
        "    \"[Content & Formats > Content Packaging] Titles, Thumbnails\",\n",
        "    \"[Content & Formats > Content Packaging] Hooks\",\n",
        "    \"[Content & Formats > Content Packaging] Chapters\",\n",
        "    \"[Content & Formats > Content Packaging] Video Structure Strategy\",\n",
        "\n",
        "    # Tools and Policy\n",
        "    \"[Tools and Policy > Copyright and content] Claims vs Strikes\",\n",
        "    \"[Tools and Policy > Copyright and content] Disputes\",\n",
        "    \"[Tools and Policy > Copyright and content] Copyright Risk Management\",\n",
        "    \"[Tools and Policy > Brand Safety and Ads] Yellow Icon\",\n",
        "    \"[Tools and Policy > Brand Safety and Ads] Advertiser Suitability\",\n",
        "    \"[Tools and Policy > Brand Safety and Ads] Self Certification\",\n",
        "    \"[Tools and Policy > Brand Safety and Ads] Ad Restrictions\",\n",
        "\n",
        "    # Creator Health and Ops\n",
        "    \"[Creator Health and Ops > Sustainability and Ops] Burnout\",\n",
        "    \"[Creator Health and Ops > Sustainability and Ops] Upload Cadence Stress\",\n",
        "    \"[Creator Health and Ops > Sustainability and Ops] Team Scaling\",\n",
        "    \"[Creator Health and Ops > Sustainability and Ops] Prod Workflow Strain\",\n",
        "\n",
        "    # Relationship and Strategic Support\n",
        "    \"[Relationship and Strategic Support > Strategic Partnership and Support] Feedback on SPM support\",\n",
        "    \"[Relationship and Strategic Support > Strategic Partnership and Support] Need for Escalation\",\n",
        "    \"[Relationship and Strategic Support > Strategic Partnership and Support] Milestone Logistics\",\n",
        "    \"[Relationship and Strategic Support > Strategic Partnership and Support] Awards/Events\",\n",
        "\n",
        "    # Analytics and Growth\n",
        "    \"[Analytics and Growth > Retention and Discovery] Audience Retention Curves\",\n",
        "    \"[Analytics and Growth > Retention and Discovery] Returning Viewers\",\n",
        "    \"[Analytics and Growth > Retention and Discovery] Engagement Depth\",\n",
        "    \"[Analytics and Growth > Retention and Discovery] Subscriber Conversion\",\n",
        "    \"[Analytics and Growth > Traffic and Discovery] Browse vs Search vs Suggested\",\n",
        "    \"[Analytics and Growth > Traffic and Discovery] Growth Volatility\",\n",
        "    \"[Analytics and Growth > Traffic and Discovery] Traffic Source Dependency\",\n",
        "    \"[Analytics and Growth > Topic and Demand Discovery] Research Tab\",\n",
        "    \"[Analytics and Growth > Topic and Demand Discovery] Keyword Demand\",\n",
        "    \"[Analytics and Growth > Topic and Demand Discovery] Content Ideation off Audience\",\n",
        "    \"[Analytics and Growth > Topic and Demand Discovery] Search Trends Buff\",\n",
        "    \"[Analytics and Growth > Performance Metric Interpretation] Confusion/Discussion on RPM vs CPM\",\n",
        "    \"[Analytics and Growth > Performance Metric Interpretation] Impression\",\n",
        "    \"[Analytics and Growth > Performance Metric Interpretation] CTR\",\n",
        "    \"[Analytics and Growth > Performance Metric Interpretation] Analytics Insights to Gaps\"\n",
        "]\n",
        "\n",
        "SCENARIOS = [\n",
        "    {\"type\": \"Escalation & Crisis Resolution\", \"duration_minutes\": 45, \"focus\": \"Urgent issue resolution\", \"chaos_probability\": 0.6},\n",
        "    {\"type\": \"Quarterly Business Review\", \"duration_minutes\": 60, \"focus\": \"Strategic planning\", \"chaos_probability\": 0.2},\n",
        "    {\"type\": \"Annual Strategy Workshop\", \"duration_minutes\": 90, \"focus\": \"Long-term growth & multiple product adoption\", \"chaos_probability\": 0.1},\n",
        "    {\"type\": \"Content Strategy Deep Dive\", \"duration_minutes\": 120, \"focus\": \"Comprehensive content overhaul\", \"chaos_probability\": 0.3}\n",
        "]\n",
        "\n",
        "CHAOS_INSTRUCTIONS = {\n",
        "    \"Technical Glitch\": \"The Creator's audio crackles. They have to repeat themselves. The SPM sounds slightly annoyed but hides it.\",\n",
        "    \"Interruption\": \"A loud background noise (dog, doorbell) forces a 2-turn pause. The conversation loses its momentum.\",\n",
        "    \"Jargon Misunderstanding\": \"The Creator thinks 'RPM' is 'Revenue Per Million' and gets excited; the SPM must correct the math.\",\n",
        "    \"Tangent\": \"The Creator spends 3 turns complaining about a recent movie or the weather. The SPM has to struggle to bring the conversation back to business.\",\n",
        "    \"Budget Stress\": \"The Creator focuses obsessively on 'how much this costs' and ignores the security/policy benefits for several turns.\",\n",
        "    \"None\": \"Standard professional flow with minimal verbal fillers.\"\n",
        "}\n",
        "\n",
        "# --- NUEVA LISTA DE SPMs CON IDs √öNICOS ---\n",
        "SPM_DB = [{\"name\": fake.name(), \"id\": f\"SPM_{uuid.uuid4().hex[:6].upper()}\"} for _ in range(20)]\n",
        "\n",
        "NICHES = [\n",
        "    # Originals\n",
        "    \"Gaming\", \"Beauty\", \"Tech Review\", \"Finance\", \"Vlog/Lifestyle\", \"Cooking\",\n",
        "\n",
        "    # Entertainment & Culture\n",
        "    \"True Crime\", \"Comedy/Sketch\", \"Movie/TV Reviews\", \"Commentary/Drama\", \"Animation\", \"ASMR\",\n",
        "\n",
        "    # Education & Personal Development\n",
        "    \"Education/Edutainment\", \"Productivity\", \"Self-Help/Motivation\", \"BookTube/Literature\", \"Language Learning\",\n",
        "\n",
        "    # Health, Wellness & Sports\n",
        "    \"Fitness/Workout\", \"Mental Health/Wellness\", \"Sports/Highlights\",\n",
        "\n",
        "    # Hobbies & Skills\n",
        "    \"DIY/Crafts\", \"Art/Drawing\", \"Music Production/Covers\", \"Photography/Videography\", \"Automotive/Cars\", \"Gardening\",\n",
        "\n",
        "    # Specific Lifestyle\n",
        "    \"Travel\", \"Fashion\", \"Parenting/Family\", \"Pets/Animals\", \"Real Estate\", \"Minimalism\"\n",
        "]\n",
        "\n",
        "CHANNEL_PREFIXES = {\n",
        "    # Originals\n",
        "    \"Gaming\": [\"Pixel\", \"Neon\", \"Retro\", \"Speed\", \"Shadow\", \"Elite\", \"Pro\", \"Quest\", \"Cyber\", \"Glitch\"],\n",
        "    \"Beauty\": [\"Glow\", \"Pure\", \"Luxe\", \"Velvet\", \"Chic\", \"Radiant\", \"Bella\", \"Silk\", \"Trend\", \"Glam\"],\n",
        "    \"Tech Review\": [\"Future\", \"Smart\", \"Tech\", \"Binary\", \"Gadget\", \"Silicon\", \"Digital\", \"Byte\", \"Hardware\", \"Logic\"],\n",
        "    \"Finance\": [\"Wealth\", \"Market\", \"Crypto\", \"Asset\", \"Value\", \"Capital\", \"Bull\", \"Fiscal\", \"Invest\", \"Money\"],\n",
        "    \"Vlog/Lifestyle\": [\"Daily\", \"Urban\", \"Wild\", \"Simple\", \"Happy\", \"Travel\", \"LifeWith\", \"Vibe\", \"Core\", \"Just\"],\n",
        "    \"Cooking\": [\"Tasty\", \"Chef\", \"Golden\", \"Spicy\", \"Fresh\", \"Yummy\", \"Kitchen\", \"Baked\", \"Savory\", \"Sweet\"],\n",
        "\n",
        "    # Entertainment & Culture\n",
        "    \"True Crime\": [\"Dark\", \"Cold\", \"Mystery\", \"Night\", \"Shadow\", \"Silent\", \"Crime\", \"Deep\", \"Hidden\", \"Case\"],\n",
        "    \"Comedy/Sketch\": [\"Funny\", \"Laugh\", \"Joker\", \"Sketch\", \"HaHa\", \"Epic\", \"Wild\", \"Crazy\", \"Chuckle\", \"Giggle\"],\n",
        "    \"Movie/TV Reviews\": [\"Screen\", \"Film\", \"Cinema\", \"Plot\", \"Binge\", \"Review\", \"Frame\", \"Movie\", \"Scene\", \"Roll\"],\n",
        "    \"Commentary/Drama\": [\"Tea\", \"Spill\", \"Topic\", \"Point\", \"Open\", \"Real\", \"Truth\", \"Talk\", \"Voice\", \"Fair\"],\n",
        "    \"Animation\": [\"Toon\", \"Ink\", \"Draw\", \"Frame\", \"Vector\", \"Pixel\", \"Motion\", \"Sketch\", \"Doodle\", \"Art\"],\n",
        "    \"ASMR\": [\"Soft\", \"Quiet\", \"Tingle\", \"Gentle\", \"Pure\", \"Zen\", \"Calm\", \"Relax\", \"Deep\", \"Echo\"],\n",
        "\n",
        "    # Education & Personal Development\n",
        "    \"Education/Edutainment\": [\"Smart\", \"Learn\", \"Brain\", \"Fact\", \"Know\", \"Mind\", \"Study\", \"Bright\", \"Quick\", \"Pure\"],\n",
        "    \"Productivity\": [\"Focus\", \"Prime\", \"Flow\", \"Efficient\", \"Peak\", \"Done\", \"Work\", \"Plan\", \"Swift\", \"Method\"],\n",
        "    \"Self-Help/Motivation\": [\"Rise\", \"Growth\", \"Vibe\", \"Soul\", \"Path\", \"Goal\", \"Strong\", \"Ever\", \"Higher\", \"Will\"],\n",
        "    \"BookTube/Literature\": [\"Page\", \"Novel\", \"Chapter\", \"Ink\", \"Book\", \"Shelf\", \"Read\", \"Verse\", \"Text\", \"Story\"],\n",
        "    \"Language Learning\": [\"Fluent\", \"Speak\", \"Word\", \"Lingo\", \"Poly\", \"Talk\", \"Global\", \"Native\", \"Bridge\", \"Key\"],\n",
        "\n",
        "    # Health, Wellness & Sports\n",
        "    \"Fitness/Workout\": [\"Iron\", \"Core\", \"Flex\", \"Power\", \"Active\", \"Fit\", \"Grind\", \"Pulse\", \"Strong\", \"Titan\"],\n",
        "    \"Mental Health/Wellness\": [\"Soul\", \"Peace\", \"Calm\", \"Mindful\", \"Zen\", \"Heart\", \"Heal\", \"Space\", \"Balance\", \"Clear\"],\n",
        "    \"Sports/Highlights\": [\"Game\", \"Pro\", \"Goal\", \"Sport\", \"Fast\", \"Clutch\", \"Apex\", \"Field\", \"Court\", \"Fan\"],\n",
        "\n",
        "    # Hobbies & Skills\n",
        "    \"DIY/Crafts\": [\"Handy\", \"Make\", \"Build\", \"Craft\", \"Home\", \"Tool\", \"Create\", \"Design\", \"Fix\", \"Pro\"],\n",
        "    \"Art/Drawing\": [\"Ink\", \"Brush\", \"Canvas\", \"Color\", \"Palette\", \"Stroke\", \"Art\", \"Visual\", \"Hue\", \"Paint\"],\n",
        "    \"Music Production/Covers\": [\"Beat\", \"Sound\", \"Audio\", \"Note\", \"Studio\", \"Vocal\", \"Melody\", \"Track\", \"Wave\", \"Mix\"],\n",
        "    \"Photography/Videography\": [\"Lens\", \"Shutter\", \"Frame\", \"Capture\", \"View\", \"Focus\", \"Angle\", \"Snap\", \"Raw\", \"Flash\"],\n",
        "    \"Automotive/Cars\": [\"Turbo\", \"Shift\", \"Drive\", \"Motor\", \"Engine\", \"Auto\", \"Gear\", \"Torque\", \"Piston\", \"Race\"],\n",
        "    \"Gardening\": [\"Green\", \"Root\", \"Leaf\", \"Bloom\", \"Soil\", \"Nature\", \"Wild\", \"Seed\", \"Farm\", \"Eco\"],\n",
        "\n",
        "    # Specific Lifestyle\n",
        "    \"Travel\": [\"Wander\", \"Nomad\", \"Route\", \"Global\", \"Trip\", \"Atlas\", \"Vista\", \"Way\", \"Map\", \"Bound\"],\n",
        "    \"Fashion\": [\"Style\", \"Trend\", \"Vogue\", \"Luxe\", \"Fit\", \"Chic\", \"Mode\", \"Clout\", \"Drip\", \"Look\"],\n",
        "    \"Parenting/Family\": [\"Home\", \"Nest\", \"Kind\", \"Parent\", \"Little\", \"Sweet\", \"Daily\", \"Joy\", \"Root\", \"Life\"],\n",
        "    \"Pets/Animals\": [\"Paws\", \"Wild\", \"Tail\", \"Pet\", \"Fur\", \"Bark\", \"Cute\", \"Safe\", \"Critter\", \"Nature\"],\n",
        "    \"Real Estate\": [\"Pro\", \"Estate\", \"Home\", \"Land\", \"Key\", \"Prime\", \"Urban\", \"Metro\", \"Yield\", \"Prop\"],\n",
        "    \"Minimalism\": [\"Pure\", \"Simple\", \"Less\", \"Clean\", \"Zen\", \"Blank\", \"Core\", \"Base\", \"Sleek\", \"Plain\"]\n",
        "}\n",
        "\n",
        "CHANNEL_SUFFIXES = {\n",
        "    # Originals\n",
        "    \"Gaming\": [\"Plays\", \"Gaming\", \"Arcade\", \"Zone\", \"Quest\", \"TV\", \"Live\", \"Station\", \"Hub\", \"Verse\"],\n",
        "    \"Beauty\": [\"Beauty\", \"Cosmetics\", \"Skin\", \"Style\", \"Makeup\", \"Looks\", \"Secrets\", \"Studio\", \"Room\", \"Diaries\"],\n",
        "    \"Tech Review\": [\"Reviews\", \"Lab\", \"Unboxed\", \"Flow\", \"Hub\", \"Central\", \"Insights\", \"Talks\", \"Breakdown\", \"Zone\"],\n",
        "    \"Finance\": [\"Watch\", \"Flow\", \"Capital\", \"Sense\", \"Moves\", \"Tips\", \"Guru\", \"Strategies\", \"Roadmap\", \"Hustle\"],\n",
        "    \"Vlog/Lifestyle\": [\"Vlogs\", \"Life\", \"Journeys\", \"Adventures\", \"Stories\", \"Days\", \"Moments\", \"World\", \"Lens\", \"Focus\"],\n",
        "    \"Cooking\": [\"Kitchen\", \"Eats\", \"Bites\", \"Table\", \"Recipes\", \"Bakery\", \"Cooks\", \"Flavors\", \"Delights\", \"Spot\"],\n",
        "\n",
        "    # Entertainment & Culture\n",
        "    \"True Crime\": [\"Files\", \"Vault\", \"Tales\", \"Secrets\", \"Chronicles\", \"Reports\", \"Incident\", \"Theory\", \"Investigation\", \"Records\"],\n",
        "    \"Comedy/Sketch\": [\"Show\", \"Laughs\", \"Comedy\", \"Sketches\", \"Humor\", \"Jokes\", \"Fun\", \"Gags\", \"Vibes\", \"Stuff\"],\n",
        "    \"Movie/TV Reviews\": [\"Box\", \"Reel\", \"Critic\", \"Review\", \"Analysis\", \"Fix\", \"Central\", \"Guide\", \"Tube\", \"Watch\"],\n",
        "    \"Commentary/Drama\": [\"Tea\", \"Central\", \"Opinions\", \"Talk\", \"Drama\", \"Exposed\", \"Buzz\", \"DeepDive\", \"News\", \"Reality\"],\n",
        "    \"Animation\": [\"Studio\", \"Toons\", \"Anims\", \"Shorts\", \"World\", \"Design\", \"Creation\", \"Works\", \"Box\", \"Lab\"],\n",
        "    \"ASMR\": [\"Tingles\", \"Whispers\", \"Sleep\", \"Relax\", \"Therapy\", \"Sound\", \"Calm\", \"Vibes\", \"Zen\", \"Zone\"],\n",
        "\n",
        "    # Education & Personal Development\n",
        "    \"Education/Edutainment\": [\"University\", \"Facts\", \"Explained\", \"101\", \"Hub\", \"Academy\", \"Class\", \"Lab\", \"School\", \"Portal\"],\n",
        "    \"Productivity\": [\"System\", \"Methods\", \"Growth\", \"Hacks\", \"Journal\", \"Pro\", \"Labs\", \"Mastery\", \"Success\", \"Way\"],\n",
        "    \"Self-Help/Motivation\": [\"Mindset\", \"Daily\", \"Vision\", \"Impact\", \"Path\", \"Way\", \"Journey\", \"Life\", \"Core\", \"Rise\"],\n",
        "    \"BookTube/Literature\": [\"Shelf\", \"Reads\", \"Library\", \"Review\", \"Lovers\", \"Corner\", \"Nook\", \"Verse\", \"Pages\", \"World\"],\n",
        "    \"Language Learning\": [\"Method\", \"Course\", \"Talk\", \"Path\", \"Way\", \"Fluent\", \"Academy\", \"Bridge\", \"World\", \"Steps\"],\n",
        "\n",
        "    # Health, Wellness & Sports\n",
        "    \"Fitness/Workout\": [\"Gym\", \"Fit\", \"Performance\", \"Coaching\", \"Muscle\", \"Results\", \"System\", \"Flow\", \"Life\", \"Training\"],\n",
        "    \"Mental Health/Wellness\": [\"Healing\", \"Mind\", \"Spirit\", \"Peace\", \"Wellness\", \"Daily\", \"Path\", \"Soul\", \"Well\", \"Center\"],\n",
        "    \"Sports/Highlights\": [\"Highlights\", \"TV\", \"Fan\", \"Replay\", \"Clips\", \"Scout\", \"Report\", \"Talk\", \"Hub\", \"Zone\"],\n",
        "\n",
        "    # Hobbies & Skills\n",
        "    \"DIY/Crafts\": [\"Fix\", \"Made\", \"Workshop\", \"Project\", \"Ideas\", \"Craft\", \"Home\", \"Builds\", \"Solutions\", \"Lab\"],\n",
        "    \"Art/Drawing\": [\"Art\", \"Gallery\", \"Sketchbook\", \"Studio\", \"Draws\", \"Ink\", \"Portfolio\", \"Workshop\", \"Space\", \"Concept\"],\n",
        "    \"Music Production/Covers\": [\"Music\", \"Records\", \"Beats\", \"Covers\", \"Sound\", \"Studio\", \"Tracks\", \"Session\", \"Mix\", \"Production\"],\n",
        "    \"Photography/Videography\": [\"Photo\", \"Video\", \"Media\", \"Visuals\", \"Productions\", \"Studio\", \"Lens\", \"Shoots\", \"Academy\", \"Lab\"],\n",
        "    \"Automotive/Cars\": [\"Garage\", \"Mods\", \"Review\", \"Spec\", \"Works\", \"Drive\", \"Customs\", \"Builds\", \"Performance\", \"Hub\"],\n",
        "    \"Gardening\": [\"Garden\", \"Farm\", \"Plants\", \"Land\", \"Nature\", \"Green\", \"Roots\", \"Yard\", \"Acres\", \"Patch\"],\n",
        "\n",
        "    # Specific Lifestyle\n",
        "    \"Travel\": [\"Travels\", \"Explores\", \"Wander\", \"Nomad\", \"Diary\", \"Vlogs\", \"Trip\", \"World\", \"Adventure\", \"Way\"],\n",
        "    \"Fashion\": [\"Hauls\", \"Style\", \"Edit\", \"Fashion\", \"Looks\", \"Collection\", \"Trends\", \"Wardrobe\", \"Wear\", \"Fit\"],\n",
        "    \"Parenting/Family\": [\"Family\", \"Kids\", \"Mom\", \"Dad\", \"Life\", \"Diaries\", \"Days\", \"Nest\", \"Home\", \"Chaos\"],\n",
        "    \"Pets/Animals\": [\"Pets\", \"Dogs\", \"Cats\", \"Wild\", \"Rescue\", \"Life\", \"Care\", \"World\", \"Tails\", \"Buddies\"],\n",
        "    \"Real Estate\": [\"Investing\", \"Prop\", \"Homes\", \"Market\", \"Advisors\", \"Yield\", \"Strategies\", \"Living\", \"Metro\", \"View\"],\n",
        "    \"Minimalism\": [\"Life\", \"Simple\", \"Essentials\", \"Living\", \"Mind\", \"Way\", \"Home\", \"Space\", \"Minimal\", \"Method\"]\n",
        "}\n",
        "\n",
        "REGIONS = {\n",
        "    \"North America (USA)\": {\n",
        "        \"code\": \"en-US\",\n",
        "        \"style\": \"Direct, energetic, uses American idioms (bucks, awesome, dude).\",\n",
        "        \"cultural_context\": \"Western business casual.\"\n",
        "    },\n",
        "    \"Europe (UK)\": {\n",
        "        \"code\": \"en-GB\",\n",
        "        \"style\": \"Polite, perhaps slightly dry humor, uses British terms (cheers, mate, brilliant).\",\n",
        "        \"cultural_context\": \"European formality mixed with wit.\"\n",
        "    },\n",
        "    \"Asia Pacific (India)\": {\n",
        "        \"code\": \"en-IN\",\n",
        "        \"style\": \"Respectful, expressive, formal but warm, uses specific Indian English phrasing.\",\n",
        "        \"cultural_context\": \"High deference to authority/policy.\"\n",
        "    },\n",
        "    \"Latin America (Brazil)\": {\n",
        "        \"code\": \"en-BR\",\n",
        "        \"style\": \"Warm, engaging, enthusiastic, speaks English with a slight Portuguese cadence or phrasing.\",\n",
        "        \"cultural_context\": \"Relationship-focused.\"\n",
        "    },\n",
        "    \"Europe (Germany)\": {\n",
        "        \"code\": \"en-DE\",\n",
        "        \"style\": \"Direct, precise, efficient, less small talk, focuses on facts.\",\n",
        "        \"cultural_context\": \"Efficiency-focused.\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "rZG3lSHy9nL-"
      },
      "id": "rZG3lSHy9nL-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Helper Functions"
      ],
      "metadata": {
        "id": "UjF1In8u9qAM"
      },
      "id": "UjF1In8u9qAM"
    },
    {
      "cell_type": "code",
      "id": "niFtBbXQkiRSCB49okHSalaD",
      "metadata": {
        "tags": [],
        "id": "niFtBbXQkiRSCB49okHSalaD"
      },
      "source": [
        "# ==========================================\n",
        "# 3. HELPER FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def parse_and_clean_transcript(raw_text: str, start_time: datetime, expected_duration_minutes: int) -> Tuple[List[Dict[str, str]], str]:\n",
        "    \"\"\"\n",
        "    Data Cleansing at Source: Parses the LLM output, fixes structural errors,\n",
        "    and returns a strict list of {\"role\": \"...\", \"content\": \"...\"} objects.\n",
        "    \"\"\"\n",
        "    dialogue_list = []\n",
        "\n",
        "    try:\n",
        "        # 1. Limpieza de Markdown y Double Escaping\n",
        "        clean_text = raw_text\n",
        "        clean_text = re.sub(r\"^```(?:json)?\\s*\", \"\", clean_text)\n",
        "        clean_text = re.sub(r\"\\s*```$\", \"\", clean_text)\n",
        "        clean_text = re.sub(r'\"\"(role|content)\"\"', r'\"\\1\"', clean_text)\n",
        "\n",
        "        dialogue_list_raw = json.loads(clean_text)\n",
        "\n",
        "        if not isinstance(dialogue_list_raw, list):\n",
        "            raise ValueError(\"Parsed JSON is not an array.\")\n",
        "\n",
        "        # 2. Repair Logic (Split-Object Hallucination)\n",
        "        current_speaker = None\n",
        "        for item in dialogue_list_raw:\n",
        "            if \"role\" in item and \"content\" in item:\n",
        "                # Caso ideal\n",
        "                if item[\"role\"] in [\"SPM\", \"Creator\", \"Action\"] and item[\"role\"] != \"role\":\n",
        "                    dialogue_list.append({\"role\": item[\"role\"], \"content\": item[\"content\"]})\n",
        "                    current_speaker = None\n",
        "                    continue\n",
        "\n",
        "                # Rescate de Split-Object\n",
        "                if item[\"role\"] == \"role\" or item.get(\"content\") in [\"SPM\", \"Creator\"]:\n",
        "                    current_speaker = item.get(\"content\")\n",
        "                    if current_speaker not in [\"SPM\", \"Creator\", \"Action\"]:\n",
        "                        current_speaker = None\n",
        "                elif item[\"role\"] == \"content\" and current_speaker:\n",
        "                    dialogue_list.append({\"role\": current_speaker, \"content\": item[\"content\"]})\n",
        "                    current_speaker = None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è JSON Parsing Error (Fallback active): {e}\")\n",
        "        dialogue_list = []\n",
        "\n",
        "        # --- NEW REGEX FOR BROKEN JSON ---\n",
        "        # Matches the pattern \"role\": \"X\", \"content\": \"Y\" ignoring line breaks and structural errors\n",
        "        pattern = r'\"role\"\\s*:\\s*\"([^\"]+)\"\\s*,\\s*\"content\"\\s*:\\s*\"(.*?)(?<!\\\\)\"'\n",
        "\n",
        "        matches = re.finditer(pattern, raw_text, re.DOTALL | re.IGNORECASE)\n",
        "        for match in matches:\n",
        "            role_found = match.group(1).strip()\n",
        "            content_found = match.group(2).strip()\n",
        "\n",
        "            # Normalize Role\n",
        "            if \"spm\" in role_found.lower():\n",
        "                role_found = \"SPM\"\n",
        "            elif \"creator\" in role_found.lower():\n",
        "                role_found = \"Creator\"\n",
        "            else:\n",
        "                continue # Ignore garbage matches\n",
        "\n",
        "            # Clean escaped line breaks from the LLM\n",
        "            content_found = content_found.replace('\\\\n', '\\n').replace('\\\\\"', '\"')\n",
        "\n",
        "            dialogue_list.append({\"role\": role_found, \"content\": content_found})\n",
        "\n",
        "    # 3. C√°lculo de duraci√≥n total de la llamada (manteniendo la varianza)\n",
        "    variance_factor = random.uniform(0.90, 1.10)\n",
        "    total_seconds = int(expected_duration_minutes * 60 * variance_factor)\n",
        "    end_time_iso = (start_time + timedelta(seconds=total_seconds)).isoformat()\n",
        "\n",
        "    return dialogue_list, end_time_iso\n",
        "\n",
        "\n",
        "def normalize_record_for_jsonl(rec: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Clean up types for BigQuery ingestion.\n",
        "    Simplified to remove validation specific fields.\n",
        "    \"\"\"\n",
        "    normalized = rec.copy()\n",
        "\n",
        "    for k in (\"recording_start\", \"recording_end\"):\n",
        "        v = normalized.get(k)\n",
        "        if isinstance(v, datetime):\n",
        "            normalized[k] = v.isoformat()\n",
        "        else:\n",
        "            try:\n",
        "                datetime.fromisoformat(v)\n",
        "                normalized[k] = v\n",
        "            except Exception:\n",
        "                normalized[k] = \"\"\n",
        "\n",
        "    try:\n",
        "        normalized[\"duration_minutes\"] = float(normalized.get(\"duration_minutes\", 0.0))\n",
        "    except Exception:\n",
        "        normalized[\"duration_minutes\"] = 0.0\n",
        "\n",
        "    return normalized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Generation Engine"
      ],
      "metadata": {
        "id": "_drXH0CD9s-b"
      },
      "id": "_drXH0CD9s-b"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. GENERATION ENGINE\n",
        "# ==========================================\n",
        "\n",
        "class CreatorAgent:\n",
        "    def __init__(self, persona_name: str):\n",
        "        # Unique ID: Combination of timestamp and hash to avoid collisions (e.g., C_173000_A1B2C3)\n",
        "        unique_hash = uuid.uuid4().hex[:6].upper()\n",
        "        self.id = f\"C_{int(time.time())}_{unique_hash}\"\n",
        "\n",
        "        self.persona_name = persona_name\n",
        "        self.persona_data = PERSONAS[persona_name]\n",
        "        self.niche = random.choice(NICHES)\n",
        "        self.experience_level = self.persona_data.get(\"experience_level\", \"intermediate\")\n",
        "        self.subs = random.randint(self.persona_data['metrics']['subs_range'][0], self.persona_data['metrics']['subs_range'][1])\n",
        "\n",
        "        self.region_name = random.choice(list(REGIONS.keys()))\n",
        "        self.region_data = REGIONS[self.region_name]\n",
        "\n",
        "        prefix = random.choice(CHANNEL_PREFIXES[self.niche])\n",
        "        suffix = random.choice(CHANNEL_SUFFIXES[self.niche])\n",
        "        unique_tail = str(random.randint(1, 999)) if random.random() < 0.6 else fake.word().capitalize()\n",
        "        self.channel_name = f\"{prefix}{suffix}{unique_tail}\"\n",
        "\n",
        "        self.creator_display_name = fake.name()\n",
        "\n",
        "        # Assigning the SPM using the new SPM_DB\n",
        "        assigned_spm = random.choice(SPM_DB)\n",
        "        self.assigned_spm_name = assigned_spm[\"name\"]\n",
        "        self.assigned_spm_id = assigned_spm[\"id\"]\n",
        "\n",
        "        self.history: List[str] = []\n",
        "\n",
        "class SimulationEngine:\n",
        "    def __init__(self, target_rows=10):\n",
        "        self.target_rows = target_rows\n",
        "        self.transcript_db = []\n",
        "\n",
        "    def generate_system_prompt(self, creator, scenario, chaos_key, topic_list, is_followup):\n",
        "        history_context = f\"PREVIOUS CONTEXT: {creator.history[-1]}\" if is_followup and creator.history else \"This is the first call.\"\n",
        "        chaos_instruction = CHAOS_INSTRUCTIONS.get(chaos_key, \"No interruptions.\")\n",
        "        duration = scenario.get('duration_minutes', 60)\n",
        "\n",
        "        # Calculate turns based on duration (approx 1.2 turns per minute for long calls to stay within token limits)\n",
        "        target_turns = int(duration * 1.2)\n",
        "\n",
        "        # Format the list of topics into a bulleted string\n",
        "        agenda_items = \"\\n\".join([f\"   - {t}\" for t in topic_list])\n",
        "\n",
        "        # Dynamic Structure Guide based on DURATION\n",
        "        if duration >= 90: # Very Long Call (1.5 - 2 hours)\n",
        "            structure_guide = f\"\"\"\n",
        "            **MANDATORY STRUCTURE (DEEP DIVE CALL - {duration} mins):**\n",
        "            1. [0-10 min] Warm-up, relationship building, and catching up on personal life/culture.\n",
        "            2. [10-20 min] Channel Health Check: Reviewing analytics (Views, RPM, Subscribers).\n",
        "            3. [20-80 min] **THE AGENDA (Core Discussion):** Discuss the following topics sequentially. Allow natural transitions between them:\n",
        "            {agenda_items}\n",
        "            4. [80-100 min] Brainstorming session for future content ideas based on these topics.\n",
        "            5. [100-{duration} min] Defining clear action items and closing.\n",
        "            \"\"\"\n",
        "        elif duration >= 45: # Standard QBR (45-60 mins)\n",
        "            structure_guide = f\"\"\"\n",
        "            **MANDATORY STRUCTURE (STRATEGY CALL - {duration} mins):**\n",
        "            1. [0-5 min] Intro and check-in.\n",
        "            2. [5-15 min] Performance Review.\n",
        "            3. [15-{duration-10} min] **Main Topics:** Cover these points in detail:\n",
        "            {agenda_items}\n",
        "            4. [{duration-10}-{duration} min] Q&A and Closing.\n",
        "            \"\"\"\n",
        "        else: # Short Call (Crisis)\n",
        "            structure_guide = f\"\"\"\n",
        "            **MANDATORY STRUCTURE (SHORT/CRISIS CALL - {duration} mins):**\n",
        "            1. [0-5 min] Immediate Triage: Address the main crisis ({topic_list[0]}).\n",
        "            2. [5-{duration-5} min] Brief check on secondary topics if time permits:\n",
        "            {agenda_items}\n",
        "            3. [{duration-5}-{duration} min] Wrap up.\n",
        "            \"\"\"\n",
        "\n",
        "        realism_guidelines = \"\"\"\n",
        "        HUMAN REALISM GUIDELINES:\n",
        "        - Add false starts, partial overlaps, self-interruptions.\n",
        "        - Include jargon misunderstanding.\n",
        "        - Allow topic drift.\n",
        "        - Make it feel recorded live, not scripted.\n",
        "        \"\"\"\n",
        "\n",
        "        # BLOCK: JSON INTEGRITY RULES\n",
        "        # Explicitly forbids the split-object hallucination observed in the incident report.\n",
        "        json_integrity_rules = \"\"\"\n",
        "        **CRITICAL JSON FORMATTING RULES (STRICT ENFORCEMENT):**\n",
        "        1. **NO SPLIT OBJECTS:** Do NOT separate the role and the content into different objects.\n",
        "           - ‚ùå INCORRECT: [{\"role\": \"role\", \"content\": \"SPM\"}, {\"role\": \"content\", \"content\": \"Hello\"}]\n",
        "           - ‚úÖ CORRECT:   [{\"role\": \"SPM\", \"content\": \"Hello\"}]\n",
        "        2. **NO DOUBLE QUOTES ON KEYS:** Do not use CSV-style double escaping.\n",
        "           - ‚ùå INCORRECT: [{\"\"role\"\": \"\"SPM\"\"}]\n",
        "           - ‚úÖ CORRECT:   [{\"role\": \"SPM\"}]\n",
        "        3. **SINGLE OBJECT PER TURN:** Each list item must contain BOTH \"role\" and \"content\".\n",
        "        4. **NO DOUBLE QUOTES IN CONTENT:** Do not use double quotes (\") within the content strings. If quotation is needed, use single quotes (') instead. This ensures the JSON remains parsable.\n",
        "        \"\"\"\n",
        "\n",
        "        return f\"\"\"\n",
        "        Generate a realistic, verbatim call transcript between a YouTube SPM and a Creator.\n",
        "\n",
        "        {realism_guidelines}\n",
        "\n",
        "        **ROLE 1: SPM ({creator.assigned_spm_name})**\n",
        "        - Goal: Cover the full agenda of {len(topic_list)} topics while maintaining the relationship.\n",
        "        - Tone: Professional, uses jargon (YPP, CTR), empathetic.\n",
        "\n",
        "        **ROLE 2: CREATOR ({creator.channel_name})**\n",
        "        - Persona: {creator.persona_name} ({creator.persona_data['tone']}).\n",
        "        - Niche: {creator.niche}\n",
        "        - Region: {creator.region_name} ({creator.region_data['style']})\n",
        "        - Pain Point: {creator.persona_data['pain_points'][0]}.\n",
        "\n",
        "        **SCENARIO:** {scenario['type']} ({duration} minutes).\n",
        "        **CHAOS:** {chaos_instruction}\n",
        "        **CONTEXT:** {history_context}\n",
        "\n",
        "        **TOPICS TO DISCUSS (AGENDA):**\n",
        "        {agenda_items}\n",
        "\n",
        "        {structure_guide}\n",
        "\n",
        "        {json_integrity_rules}\n",
        "\n",
        "        **INSTRUCTIONS:**\n",
        "        1. **LENGTH:** At least **{target_turns} dialogue turns**. DO NOT SUMMARIZE.\n",
        "        2. **DEPTH:** You must touch upon ALL topics listed in the Agenda.\n",
        "        3. **FORMAT:** Output a VALID JSON ARRAY:\n",
        "           [\n",
        "             {{\"role\": \"SPM\", \"content\": \"...\"}},\n",
        "             {{\"role\": \"Creator\", \"content\": \"...\"}}\n",
        "           ]\n",
        "        \"\"\"\n",
        "\n",
        "    def run(self):\n",
        "        print(f\"üöÄ Starting generation. Target: {self.target_rows} rows...\")\n",
        "\n",
        "        rows_generated = 0\n",
        "        all_possible_topics = TOPICS_CATALOG\n",
        "\n",
        "        while rows_generated < self.target_rows:\n",
        "            creator = CreatorAgent(random.choice(list(PERSONAS.keys())))\n",
        "            num_calls = random.choices([1, 2, 3, 4], weights=[50, 30, 15, 5], k=1)[0]\n",
        "\n",
        "            print(f\"Processing {creator.channel_name} ({creator.region_name}) - Planning {num_calls} call(s)...\")\n",
        "\n",
        "            for call_idx in range(num_calls):\n",
        "                if rows_generated >= self.target_rows: break\n",
        "\n",
        "                scenario = random.choice(SCENARIOS)\n",
        "                duration = scenario.get('duration_minutes', 60)\n",
        "                chaos_key = \"None\" if random.random() > scenario['chaos_probability'] else random.choice(list(CHAOS_INSTRUCTIONS.keys()))\n",
        "\n",
        "                # --- TOPIC SELECTION LOGIC ---\n",
        "                if duration >= 120: num_topics = random.randint(12, 18)\n",
        "                elif duration >= 90: num_topics = random.randint(8, 12)\n",
        "                elif duration >= 60: num_topics = random.randint(5, 8)\n",
        "                else: num_topics = random.randint(3, 5)\n",
        "\n",
        "                selected_topics = random.sample(all_possible_topics, min(num_topics, len(all_possible_topics)))\n",
        "                primary_topic = selected_topics[0]\n",
        "                full_topic_string = \"; \".join(selected_topics)\n",
        "\n",
        "                start_dt = datetime.now() - timedelta(days=random.randint(1, 30))\n",
        "\n",
        "                # Clean ID format\n",
        "                conv_id = f\"{creator.id}_Call{call_idx + 1}\"\n",
        "\n",
        "                # --- OPTION B: RETRY LOOP (ANTI-GAPS) ---\n",
        "                MAX_RETRIES = 3\n",
        "                attempts = 0\n",
        "                call_success = False\n",
        "                quotas_attempts = 1\n",
        "\n",
        "                while attempts < MAX_RETRIES and not call_success:\n",
        "                    attempts += 1\n",
        "                    try:\n",
        "                        # 1. Generate Raw Transcript\n",
        "                        prompt = self.generate_system_prompt(creator, scenario, chaos_key, selected_topics, call_idx > 0)\n",
        "\n",
        "                        response = model.generate_content(\n",
        "                            prompt,\n",
        "                            generation_config={\n",
        "                                \"temperature\": 0.7,\n",
        "                                \"response_mime_type\": \"application/json\",\n",
        "                                \"max_output_tokens\": 8192\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "                        try:\n",
        "                            raw_transcript_json_string = response.text\n",
        "                        except ValueError:\n",
        "                            parts = response.candidates[0].content.parts\n",
        "                            raw_transcript_json_string = \"\".join([part.text for part in parts])\n",
        "\n",
        "                        # 2. Parse & Clean\n",
        "                        clean_dialogue_list, real_end_time_iso = parse_and_clean_transcript(\n",
        "                            raw_transcript_json_string,\n",
        "                            start_dt,\n",
        "                            duration\n",
        "                        )\n",
        "\n",
        "                        # --- SUCCESS EVALUATION ---\n",
        "                        if len(clean_dialogue_list) == 0:\n",
        "                            print(f\"  ‚ö†Ô∏è Attempt {attempts}/{MAX_RETRIES} failed for {conv_id} (Empty list). Retrying...\")\n",
        "                            time.sleep(1)\n",
        "                            continue # Returns to the start of the while loop to retry\n",
        "\n",
        "                        # If it reaches here, it successfully rescued data\n",
        "                        call_success = True\n",
        "\n",
        "                        # 3. Summarize\n",
        "                        summary = model.generate_content(f\"Summarize in 1 sentence: {raw_transcript_json_string[:2000]}\").text.strip()\n",
        "                        creator.history.append(summary)\n",
        "\n",
        "                        # 4. Calculate duration\n",
        "                        try:\n",
        "                            start_iso = start_dt\n",
        "                            end_iso = datetime.fromisoformat(real_end_time_iso)\n",
        "                            duration_minutes_real = round((end_iso - start_iso).total_seconds() / 60.0, 2)\n",
        "                        except Exception:\n",
        "                            duration_minutes_real = float(duration)\n",
        "\n",
        "                        # 5. Build Record\n",
        "                        record = {\n",
        "                            \"conversation_id\": conv_id,\n",
        "                            \"creator_id\": creator.id,\n",
        "                            \"channel_name\": creator.channel_name,\n",
        "                            \"creator_niche\": creator.niche,\n",
        "                            \"creator_persona\": creator.persona_name,\n",
        "                            \"creator_tone\": creator.persona_data['tone'],\n",
        "                            \"spm_id\": creator.assigned_spm_id,\n",
        "                            \"spm_name\": creator.assigned_spm_name,\n",
        "                            \"creator_region\": creator.region_name,\n",
        "                            \"language_code\": creator.region_data['code'],\n",
        "                            \"scenario\": scenario['type'],\n",
        "                            \"product_topic\": full_topic_string,\n",
        "                            \"duration_minutes\": duration_minutes_real,\n",
        "                            \"recording_start\": start_dt.isoformat(),\n",
        "                            \"recording_end\": real_end_time_iso,\n",
        "                            \"raw_transcript\": clean_dialogue_list,\n",
        "\n",
        "                            # Pre-allocated Validator Columns (NULL)\n",
        "                            \"is_valid\": None,\n",
        "                            \"quality_score\": None,\n",
        "                            \"hallucination_flag\": None,\n",
        "                            \"validation_report\": None,\n",
        "                            \"audit_timestamp\": None\n",
        "                        }\n",
        "\n",
        "                        self.transcript_db.append(record)\n",
        "                        rows_generated += 1\n",
        "\n",
        "                        if attempts > 1:\n",
        "                            print(f\"  ‚úÖ Successfully generated on attempt {attempts}: {conv_id} ({len(selected_topics)} topics)\")\n",
        "                        else:\n",
        "                            print(f\"  ‚úÖ Generated: {conv_id} ({len(selected_topics)} topics, ~{duration_minutes_real} mins)\")\n",
        "\n",
        "                        time.sleep(1)\n",
        "\n",
        "                    except ResourceExhausted as e:\n",
        "                        # If Google pauses us, we wait a few seconds to clear the minute window\n",
        "                        print(f\"  ‚è≥ Quota limit reached (ResourceExhausted). Pausing for {2**quotas_attempts} seconds...\")\n",
        "                        time.sleep(2**quotas_attempts)\n",
        "                        attempts -= 1  # We don't burn an attempt if it was a quota issue\n",
        "                        quotas_attempts += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        # For any other random API errors\n",
        "                        print(f\"  ‚ùå General API Error on attempt {attempts}/{MAX_RETRIES} for {conv_id}: {e}\")\n",
        "                        time.sleep(1)\n",
        "\n",
        "                # If it exited the while loop and call_success is still False, give up on that call\n",
        "                if not call_success:\n",
        "                    print(f\"üíÄ ABORTED: Unable to generate {conv_id} after {MAX_RETRIES} attempts. Leaving intentional gap.\")\n",
        "\n",
        "        # Final Save\n",
        "        with open(JSONL_FILE, 'w', encoding='utf-8') as f:\n",
        "            for entry in self.transcript_db:\n",
        "                clean_entry = normalize_record_for_jsonl(entry)\n",
        "                f.write(json.dumps(clean_entry, ensure_ascii=False, separators=(',', ':')) + '\\n')\n",
        "        print(f\"‚úÖ Saved {rows_generated} transcripts to: {JSONL_FILE}\")"
      ],
      "metadata": {
        "id": "UQjfNoRr9tOj"
      },
      "id": "UQjfNoRr9tOj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. BigQuery Upload"
      ],
      "metadata": {
        "id": "NzyRo15Q9w1Z"
      },
      "id": "NzyRo15Q9w1Z"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. BIGQUERY UPLOAD\n",
        "# ==========================================\n",
        "\n",
        "def upload_to_bigquery():\n",
        "    client = bigquery.Client(project=PROJECT_ID)\n",
        "    dataset_ref = client.dataset(DATASET_ID)\n",
        "\n",
        "    try:\n",
        "        client.get_dataset(dataset_ref)\n",
        "    except NotFound:\n",
        "        dataset = bigquery.Dataset(dataset_ref)\n",
        "        dataset.location = \"US\"\n",
        "        client.create_dataset(dataset)\n",
        "        print(f\"‚úÖ Created dataset {DATASET_ID}\")\n",
        "\n",
        "    table_ref = dataset_ref.table(TABLE_ID)\n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
        "\n",
        "    # --- CAMBIO CLAVE: SCHEMA EXPL√çCITO ---\n",
        "    job_config.autodetect = False\n",
        "    job_config.schema = [\n",
        "        # Base Columns\n",
        "        bigquery.SchemaField(\"conversation_id\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"creator_id\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"channel_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"creator_niche\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"creator_persona\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"creator_tone\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"spm_id\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"spm_name\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"creator_region\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"language_code\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"scenario\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"product_topic\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"duration_minutes\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"recording_start\", \"TIMESTAMP\"),\n",
        "        bigquery.SchemaField(\"recording_end\", \"TIMESTAMP\"),\n",
        "        bigquery.SchemaField(\"raw_transcript\", \"JSON\"),\n",
        "\n",
        "        # Validator Columns (Reserved / Null initially)\n",
        "        bigquery.SchemaField(\"is_valid\", \"BOOLEAN\"),\n",
        "        bigquery.SchemaField(\"quality_score\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"hallucination_flag\", \"BOOLEAN\"),\n",
        "        bigquery.SchemaField(\"validation_report\", \"JSON\"),\n",
        "        bigquery.SchemaField(\"audit_timestamp\", \"TIMESTAMP\")\n",
        "    ]\n",
        "\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "\n",
        "    with open(JSONL_FILE, \"rb\") as source_file:\n",
        "        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
        "\n",
        "    job.result()\n",
        "    print(f\"üéâ Success! Loaded {job.output_rows} rows.\")"
      ],
      "metadata": {
        "id": "gb8_PkXH9xBo"
      },
      "id": "gb8_PkXH9xBo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Main"
      ],
      "metadata": {
        "id": "e4HKuRwCNt3c"
      },
      "id": "e4HKuRwCNt3c"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    # A. Create synthetic data\n",
        "    sim = SimulationEngine(target_rows=100)\n",
        "    sim.run()\n",
        "    end_time_sd = time.perf_counter()\n",
        "\n",
        "    # B. Upload to BigQuery\n",
        "    upload_to_bigquery()\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "    duration_sd = str(timedelta(seconds=end_time_sd - start_time))\n",
        "    duration_bq = str(timedelta(seconds=end_time - end_time_sd))\n",
        "    duration_total = str(timedelta(seconds=end_time - start_time))\n",
        "\n",
        "    print(f\"Synthetic Data execution time (HH:MM:SS): {duration_sd}\")\n",
        "    print(f\"Upload to BigQuery execution time (HH:MM:SS): {duration_bq}\")\n",
        "    print(f\"Total execution time (HH:MM:SS): {duration_total}\")"
      ],
      "metadata": {
        "id": "RZtiauz0Nwwg",
        "collapsed": true
      },
      "id": "RZtiauz0Nwwg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In average it takes:\n",
        "\n",
        "~45 seconds to create a transcription\n",
        "\n",
        "~3 seconds to upload new transcriptions"
      ],
      "metadata": {
        "id": "r09m4JqmGk88"
      },
      "id": "r09m4JqmGk88"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "vel_csv_synthetic_data_generator_006",
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
